[
  {
    "objectID": "why_quarto.html",
    "href": "why_quarto.html",
    "title": "Why Quarto?",
    "section": "",
    "text": "As described in the quarto documentation: Quarto is a new, open-source, scientific, and technical publishing system. It is a multi-language, next generation version of R Markdown from RStudio, with many new features and capabilities. Like R Markdown, Quarto uses Knitr to execute R code, and is therefore able to render most existing Rmd files without modification.\nData scientists are pretty excited about the introduction of Quarto, and since it represents the future of R Markdown, we will conduct SDS 264 using Quarto. Intriguing Quarto features that have been cited include:\nFrom Posit and the developers of Rstudio and Quarto https://charlotte.quarto.pub/cascadia/, Quarto (compared to RMarkdown):\nIn other words, Quarto unifies and extends R Markdown:\nHere’s a cool example from the Quarto documentation, showing features like cross-referencing of figures, chunk options using the hash-pipe format, collapsed code, and easy figure legends:",
    "crumbs": [
      "Why Quarto?"
    ]
  },
  {
    "objectID": "why_quarto.html#air-quality",
    "href": "why_quarto.html#air-quality",
    "title": "Why Quarto?",
    "section": "Air Quality",
    "text": "Air Quality\nFigure 1 further explores the impact of temperature on ozone level.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(airquality, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"loess\")\n\n\n\n\n\n\n\n\nFigure 1: Temperature and ozone level.",
    "crumbs": [
      "Why Quarto?"
    ]
  },
  {
    "objectID": "rtipoftheday.html",
    "href": "rtipoftheday.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Signup Sheet\nHow to create cool presentations using Quarto\nCode for creating the presentation above\nMy example of an RTD presentation with revealjs in quarto\nCode for my example\nRTD rubric\n\nTIP: Create your presentation in a new project (separate from the one you created for class). You can run the presentation from your own machine OR you can publish it to the RStudio/Posit Connect server. To do this:\n\n\nRender your presentation.\nClick the ‘Publish’ icon in the upper right corner of the screen.\nSelect Posit Connect as the server to publish to.\nChoose to publish the finished product only.\nAt this point if you are doing it for the first time, you will be asked to connect to an account. Choose Posit Connect.\nEnter “https://rconnect.stolaf.edu” as the URL then click next.\nAt this point you will be redirected to a webpage to sign in. Use your stolaf account to log in and follow the instructions to connect your RStudio to the server. Go back to your desktop RStudio and click ‘Connect Account’.\nNow you are ready to publish the document. Give it a good name and click ‘Publish’.\nAfter a minute or two, you will again be redirected to a browser where your document is now available. Edit the Share settings appropriately (Anyone - no login required is fine, but be aware that others will be able to see your work). Copy the Content URL to share."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SDS 264: Data Science 2 (Fall 2025)",
    "section": "",
    "text": "Key links for SDS 264\n\nCourse syllabus\nCourse schedule\nRStudio server\nmoodle\nGitHub source code for this website"
  },
  {
    "objectID": "github_intro.html",
    "href": "github_intro.html",
    "title": "Intro to GitHub",
    "section": "",
    "text": "Version Control (GitHub)\nIn order to collaborate on an R project (or any coding project in general), data scientists typically use a version control system like GitHub. With GitHub, you never have to save files as Final.docx, Final2.docx, Newfinal.docx, RealFinal.docx, nothisisreallyit.docx, etc. You update code like .qmd and .Rmd and data files, recording descriptions of any changes. Then if you ever want to go back to an earlier version, GitHub can facilitate that. Or if you want to make your work public, others can see it and even suggest changes, but you are ultimately in control of any changes that get made.\nAnd, you can have multiple collaborators with access to the same set of files. While it can be dicey if multiple people have the file open and make changes at the same time; if you do this with GitHub, it is at least POSSIBLE to get it straightened out, and the second person who tries to save will get warned. If you are both just using a common folder on RStudio, you can easily write over and erase each other’s work. (If you use a common folder, be sure that one person is editing at a time to prevent this).\nIn order to begin to get familiar with GitHub, we will use it to create a course folder for you.\n\n\nGetting started on GitHub and connecting to RStudio\n\nCreate a GitHub account at github.com. It’s usually okay to hit “Skip Personalization” at the bottom of the screen after entering an email, username, and password (you might have to enable 2-factor authentication as well). There are a few bonuses you can get as a student that you might consider.\n\nYou may choose to use a non-St. Olaf email address to ensure you’ll have access to your GitHub account after you graduate.\n\nObtain a personal access token (PAT) in GitHub using the following steps:\n\nClick your profile picture/symbol in the upper right of your GitHub page. Then select Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic).\nClick “Generate new token (classic)” and give a descriptive name like “My PAT for RStudio”. Note that the default expiration is 30 days; I did a Custom setting through the end of the semester.\nSelect scopes and permissions; I often select: repo, workflow, gist, and user.\nClick “Generate token”. Copy your personal access token and store it somewhere.\n\nStore your credentials in RStudio using the following steps:\n\nIn the console, type library(credentials). You might have to install the credentials package first.\nThen type set_github_pat() and hit Return. You can sign in with the browser, or you can choose the Token option, where you can copy in your personal access token\n\n\nAlert! If the steps in (3) don’t work, you may have to install Git on your computer first. This chapter in “Happy Git with R” provides nice guidance for installing Git. Installing Git for Windows seems to work well on most Windows machines, using this site seems to work well for macOS, and a command like sudo apt-get install git can often work nicely in Linux. Once Git is installed, restart RStudio, and it will usually magically find Git. If not, there’s some good advice in this chapter of “Happy Git with R”. If you ever get frustrated with Git, remember that No one is giving out Git Nerd merit badges! Just muddle through until you figure out something that works for you!\n\n\nCreating an R project (local) that’s connected to GitHub (cloud)\n\nIn your GitHub account, click the \\(+ \\nabla\\) (+down arrow) button near the top right and select New Repository (repo). Put something like “SDS264_S25” for your repository (repo) name; use simple but descriptive names that avoid spaces. Check Private for now; you can turn a repository Public if you want later. Check Add a ReadMe File. Finally hit Create Repository and copy the URL once your repo has been created; the URL should be something like github.com/username/SDS264_S25.\nGo into your RStudio and select File &gt; New Project &gt; Version Control &gt; Git. For the repository URL paste in the URL for the repository you just created. A project directory named “SDS264_S25” will be created in your specified folder (which you can navigate to).\n\nNotice that you are now starting with a blank slate! Nothing in the environment or history. Also note where it says your project name in the top right corner.\nAt this point your should have a GitHub repo called “SDS264_S25” connected to an R project named “SDS264_S25”. The basic framework is set!\nHere is an illustration (source) of the process of using GitHub to manage version control and collaboration, followed by greater detail about each step:\n\n\n\nCreating new files in RStudio (local)\n\nYou can download our first file with in-class exercises here. Just hit the Download Raw File button and note where the file is saved on your computer. Use File &gt; Open File in RStudio to open up 01_review164.qmd. Then, use File &gt; Save As to navigate to the SDS264_S25 folder on your computer and save a copy there. You can even add your name or a few answers and re-save your file.\n\n\n\nMaking sure the connection to GitHub is ready\n\n[If necessary] You may need one of these two steps when using GitHub for the first name in a new R Project, even though you likely did them while installing GitHub.\n\nIn the console, type library(credentials). Then type set_github_pat(), hit Return, and copy in your personal access token.\nIn the Terminal window (this should be the tab next to Console), type the following two lines (be precise with the dashes and spaces!!):\n\n\n\ngit config --global user.name \"YOUR USER NAME\"\n\n\ngit config --global user.email \"YOUR EMAIL ASSOCIATED WITH GITHUB\"\n\n\n\nPushing your work to GitHub (cloud)\n\nWe now want to “push” the changes made in 01_review164.qmd to your GitHub repo (the changes have only been made in your local RStudio for now).\n\nunder the Git tab in the Environment panel, check the boxes in front of all modified files to “stage” your changes. To select a large number of files, check the top box, scroll down to the bottom of the list, and then shift-click the final box\nclick the Commit tab to “commit” your changes (like saving a file locally) along with a message describing the changes you made. GitHub guides you by by showing your old code (red) and new code (green), to make sure you approve of the changes. This first time, all code should be green since we’ve only added new things rather than modifying previously saved files.\n“push” your changes to GitHub (an external website) by clicking the green Up arrow. Refresh your GitHub account to see new files in the user_name.github.io repo!\n\n\n\n\nModifying files that have already been pushed to GitHub\n\nMake and save a change (anything) to your file 01_review164.qmd in RStudio. Now go back under the Git tab and “push” these new changes to GitHub. You’ll have to go through the same process of Stage, Commit, and Push, although this time you’ll see only your newest changes in green when you Commit. Confirm that your changes appear in GitHub.\n\n\n\nPulling work from GitHub\nBefore you start a new session of working on a project in RStudio, you should always Pull changes from GitHub first. Most of the time there will be nothing new, but if a collaborator made changes since the last time you worked on a file, you want to make sure you’re working with the latest and greatest version. If not, you’ll end up trying to Push changes made to an old version, and GitHub will balk and produce Merge Conflict messages. We’ll see how to handle Merge Conflicts later, but it’s a bit of a pain and best avoided!\n\nGo into 01_review164.qmd on GitHub and hit the Edit icon. Add a line anywhere, and then scroll down to hit Commit Changes. (This is not recommended and for illustrative purposes only! You will likely never edit directly in GitHub, but we’re emulating what might happen if a collaborator makes changes since the last time you worked on a document.) Now go back to RStudio and, under the Git tab, “Pull” the changes from GitHub into your R project folder. (Use the blue Down arrow). Confirm that your changes now appear in RStudio. Before you start working on the R server, you should always Pull any changes that might have been made on GitHub (especially if you’re working on a team!), since things can get dicey if you try to merge new changes from RStudio with new changes on GitHub.\n\n\n\nA bit more about R projects\n\nTo see the power of projects, select File &gt; Close Project and Don’t Save the workspace image. Then, select File &gt; Recent Projects &gt; SDS264_S25; you will get a clean Environment and Console once again, but History shows the commands you ran by hand, active Rmd and qmd files appear in the Source panel, and Files contains the Rmd, qmd, html, and csv files produced by your last session. And you can stage, commit and push the changes and the new file to GitHub!",
    "crumbs": [
      "Intro to GitHub"
    ]
  },
  {
    "objectID": "04_strings_part3.html",
    "href": "04_strings_part3.html",
    "title": "Strings: Extra Practice (Part 3)",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(httr)",
    "crumbs": [
      "Strings: Extra Practice (Part 3)"
    ]
  },
  {
    "objectID": "04_strings_part3.html#on-your-own---extra-practice-with-strings-and-regular-expressions",
    "href": "04_strings_part3.html#on-your-own---extra-practice-with-strings-and-regular-expressions",
    "title": "Strings: Extra Practice (Part 3)",
    "section": "On Your Own - Extra practice with strings and regular expressions",
    "text": "On Your Own - Extra practice with strings and regular expressions\n\nDescribe the equivalents of ?, +, * in {m,n} form.\nDescribe, in words, what the expression “(.)(.)\\2\\1” will match, and provide a word or expression as an example.\nProduce an R string which the regular expression represented by “\\..\\..\\..” matches. In other words, find a string y below that produces a TRUE in str_detect.\nSolve with str_subset(), using the words from stringr::words:\n\n\nFind all words that start or end with x.\nFind all words that start with a vowel and end with a consonant.\nFind all words that start and end with the same letter\n\n\nWhat words in stringr::words have the highest number of vowels? What words have the highest proportion of vowels? (Hint: what is the denominator?) Figure this out using the tidyverse and piping, starting with as_tibble(words) |&gt;.\nFrom the Harvard sentences data, use str_extract to produce a tibble with 3 columns: the sentence, the first word in the sentence, and the first word ending in “ed” (NA if there isn’t one).\nFind and output all contractions (words with apostrophes) in the Harvard sentences, assuming no sentence has multiple contractions.\nCarefully explain what the code below does, both line by line and in general terms.\n\n\ntemp &lt;- str_replace_all(words, \"^([A-Za-z])(.*)([a-z])$\", \"\\\\3\\\\2\\\\1\")\nas_tibble(words) |&gt;\n  semi_join(as_tibble(temp)) |&gt;\n  print(n = Inf)\n\nJoining with `by = join_by(value)`\n\n\n# A tibble: 45 × 1\n   value     \n   &lt;chr&gt;     \n 1 a         \n 2 america   \n 3 area      \n 4 dad       \n 5 dead      \n 6 deal      \n 7 dear      \n 8 depend    \n 9 dog       \n10 educate   \n11 else      \n12 encourage \n13 engine    \n14 europe    \n15 evidence  \n16 example   \n17 excuse    \n18 exercise  \n19 expense   \n20 experience\n21 eye       \n22 god       \n23 health    \n24 high      \n25 knock     \n26 lead      \n27 level     \n28 local     \n29 nation    \n30 no        \n31 non       \n32 on        \n33 rather    \n34 read      \n35 refer     \n36 remember  \n37 serious   \n38 stairs    \n39 test      \n40 tonight   \n41 transport \n42 treat     \n43 trust     \n44 window    \n45 yesterday",
    "crumbs": [
      "Strings: Extra Practice (Part 3)"
    ]
  },
  {
    "objectID": "04_strings_part3.html#coco-and-rotten-tomatoes",
    "href": "04_strings_part3.html#coco-and-rotten-tomatoes",
    "title": "Strings: Extra Practice (Part 3)",
    "section": "Coco and Rotten Tomatoes",
    "text": "Coco and Rotten Tomatoes\nWe will check out the Rotten Tomatoes page for the 2017 movie Coco, scrape information from that page (we’ll get into web scraping in a few weeks!), clean it up into a usable format, and answer some questions using strings and regular expressions.\n\n# used to work\n# coco &lt;- read_html(\"https://www.rottentomatoes.com/m/coco_2017\")\n\n# robotstxt::paths_allowed(\"https://www.rottentomatoes.com/m/coco_2017\")\n\nlibrary(polite)\ncoco &lt;- \"https://www.rottentomatoes.com/m/coco_2017\" |&gt;\n  bow() |&gt; \n  scrape()\n\ntop_reviews &lt;- \n  \"https://www.rottentomatoes.com/m/coco_2017/reviews?type=top_critics\" |&gt; \n  bow() |&gt; \n  scrape()\ntop_reviews &lt;- html_nodes(top_reviews, \".review-text\")\ntop_reviews &lt;- html_text(top_reviews)\n\nuser_reviews &lt;- \n  \"https://www.rottentomatoes.com/m/coco_2017/reviews?type=user\" |&gt; \n  bow() |&gt; \n  scrape()\nuser_reviews &lt;- html_nodes(user_reviews, \".js-review-text\")\nuser_reviews &lt;- html_text(user_reviews)\n\n\ntop_reviews is a character vector containing the 20 most recent critic reviews (along with some other junk) for Coco, while user_reviews is a character vector with the 10 most recent user reviews.\n\n\nExplain how the code below helps clean up both user_reviews and top_reviews before we start using them.\n\n\nuser_reviews &lt;- str_trim(user_reviews)\ntop_reviews &lt;- str_trim(top_reviews)\n\n\nPrint out the critic reviews where the reviewer mentions “emotion” or “cry”. Think about various forms (“cried”, “emotional”, etc.) You may want to turn reviews to all lower case before searching for matches.\nIn critic reviews, replace all instances where “Pixar” is used with its full name: “Pixar Animation Studios”.\nFind out how many times each user uses “I” in their review. Remember that it could be used as upper or lower case, at the beginning, middle, or end of a sentence, etc.\nDo critics or users have more complex reviews, as measured by average number of commas used? Be sure your code weeds out commas used in numbers, such as “12,345”.",
    "crumbs": [
      "Strings: Extra Practice (Part 3)"
    ]
  },
  {
    "objectID": "02_strings_part1.html",
    "href": "02_strings_part1.html",
    "title": "Strings: In-class Exercises (Part 1)",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis uses parts of R4DS Ch 14: Strings and Ch 15: Regular Expressions (both the first and second editions).\nlibrary(tidyverse)\n#spotify &lt;- read_csv(\"Data/spotify.csv\") \nspotify &lt;- read_csv(\"https://proback.github.io/264_fall_2025/Data/spotify.csv\")\n\nspot_smaller &lt;- spotify |&gt;\n  select(\n    title, \n    artist, \n    album_release_date, \n    album_name, \n    subgenre, \n    playlist_name\n  )\n\nspot_smaller &lt;- spot_smaller[c(5, 32, 49, 52, 83, 175, 219, 231, 246, 265), ]\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 1)"
    ]
  },
  {
    "objectID": "02_strings_part1.html#a-string-is-just-a-set-of-characters.",
    "href": "02_strings_part1.html#a-string-is-just-a-set-of-characters.",
    "title": "Strings: In-class Exercises (Part 1)",
    "section": "A string is just a set of characters.",
    "text": "A string is just a set of characters.\n\nsingle_string &lt;- \"this is a string!\"\nsingle_string\n\n[1] \"this is a string!\"\n\nstring_vector &lt;- c(\"this\", \"is\", \"a\", \"vector\", \"of strings\")\nstring_vector\n\n[1] \"this\"       \"is\"         \"a\"          \"vector\"     \"of strings\"\n\n# This is a tibble with many columns of \"string variables\", or \"character variables\"\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\n# Each column of the tibble is a vector of strings.\nspot_smaller$title\n\n [1] \"Hear Me Now\"                                      \n [2] \"Run the World (Girls)\"                            \n [3] \"Formation\"                                        \n [4] \"7/11\"                                             \n [5] \"My Oh My (feat. DaBaby)\"                          \n [6] \"It's Automatic\"                                   \n [7] \"Poetic Justice\"                                   \n [8] \"A.D.H.D\"                                          \n [9] \"Ya Estuvo\"                                        \n[10] \"Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)\"\n\n# Each item in the tibble is a string.\nspot_smaller$title[1]\n\n[1] \"Hear Me Now\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 1)"
    ]
  },
  {
    "objectID": "02_strings_part1.html#functions-that-start-str_-do-stuff-to-strings",
    "href": "02_strings_part1.html#functions-that-start-str_-do-stuff-to-strings",
    "title": "Strings: In-class Exercises (Part 1)",
    "section": "Functions that start str_ do stuff to strings!",
    "text": "Functions that start str_ do stuff to strings!\n\nstr_length()\n\n# when the input to str_length is a single string, the output is a single value:\nstr_length(\"hi\")\n\n[1] 2\n\nstr_length(single_string)\n\n[1] 17\n\n# when the input to str_length is a vector, the output is a vector:\nstr_length(string_vector)\n\n[1]  4  2  1  6 10\n\n\nstr_length takes a vector input and creates a vector output (or a single value input and returns a single value output)…. this makes it easy to use within a mutate!\n\nspot_smaller |&gt;\n  select(title) |&gt;\n  mutate(title_length = str_length(title))\n\n# A tibble: 10 × 2\n   title                                             title_length\n   &lt;chr&gt;                                                    &lt;int&gt;\n 1 Hear Me Now                                                 11\n 2 Run the World (Girls)                                       21\n 3 Formation                                                    9\n 4 7/11                                                         4\n 5 My Oh My (feat. DaBaby)                                     23\n 6 It's Automatic                                              14\n 7 Poetic Justice                                              14\n 8 A.D.H.D                                                      7\n 9 Ya Estuvo                                                    9\n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)           49\n\n\n\n\nstr_sub()\nThis function creates substrings (shorter strings)\n\n# When the input is a single string, the output is a single string\nsingle_string\n\n[1] \"this is a string!\"\n\nstr_sub(single_string, 1, 7)\n\n[1] \"this is\"\n\nstr_sub(single_string, 8, 9)\n\n[1] \" a\"\n\nstr_sub(single_string, 9, 9)\n\n[1] \"a\"\n\n# When the input is a vector of strings, what do you think the output will be?\nstring_vector\n\n[1] \"this\"       \"is\"         \"a\"          \"vector\"     \"of strings\"\n\nstr_sub(string_vector, 1, 2)\n\n[1] \"th\" \"is\" \"a\"  \"ve\" \"of\"\n\n\nHow can we use str_sub to get just the year of the album_release_date? Try it here! Then scroll down for solution.\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\n\n. . . . . . . .\n\nspot_smaller |&gt;\n  select(title, artist, album_release_date) |&gt;\n  mutate(album_release_year = str_sub(album_release_date, 1, 4))\n\n# A tibble: 10 × 4\n   title                            artist album_release_date album_release_year\n   &lt;chr&gt;                            &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;             \n 1 Hear Me Now                      Alok   2016-01-01         2016              \n 2 Run the World (Girls)            Beyon… 2011-06-24         2011              \n 3 Formation                        Beyon… 2016-04-23         2016              \n 4 7/11                             Beyon… 2014-11-24         2014              \n 5 My Oh My (feat. DaBaby)          Camil… 2019-12-06         2019              \n 6 It's Automatic                   Frees… 2013-11-28         2013              \n 7 Poetic Justice                   Kendr… 2012               2012              \n 8 A.D.H.D                          Kendr… 2011-07-02         2011              \n 9 Ya Estuvo                        Kid F… 1990-01-01         1990              \n10 Runnin (with A$AP Rocky, A$AP F… Mike … 2018-11-16         2018              \n\n\n\n\nstr_c()\nThis collapses multiple strings together into one string.\n\nstr_c(\"is\", \"this output\", \"a\", \"single value\", \"or\", \"a vector\", \"?\")\n\n[1] \"isthis outputasingle valueora vector?\"\n\n# like unite and separate, we can specify the separator:\n\nstr_c(\"is\", \"this output\", \"a\", \"single value\", \"or\", \"a vector\", \"?\", \n      sep = \" \")\n\n[1] \"is this output a single value or a vector ?\"\n\n\nWe can see that the input is a list of strings, and the output is a single string.\nSo… why is this useful?\n\nx &lt;- runif(1)\nx\n\n[1] 0.5056308\n\nstr_c(\"I can put other values, like\", x, \"in here!\", sep = \" \")\n\n[1] \"I can put other values, like 0.505630827276036 in here!\"\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\nsong_count &lt;- spot_smaller |&gt; \n  count(artist) |&gt;\n  slice_max(n, n = 1)\n\nsong_count\n\n# A tibble: 1 × 2\n  artist      n\n  &lt;chr&gt;   &lt;int&gt;\n1 Beyoncé     3\n\nsong_count$artist\n\n[1] \"Beyoncé\"\n\nsong_count$n\n\n[1] 3\n\nstr_c(\"The artist with the most songs in spot_smaller is\", song_count$artist, \"with\", song_count$n, \"songs.\", sep = \" \")\n\n[1] \"The artist with the most songs in spot_smaller is Beyoncé with 3 songs.\"\n\n\nWe can use this in a tibble too.\n\nspot_smaller |&gt;\n  select(artist, title) |&gt;\n  mutate(song_by = str_c(title, \"by\", artist, sep = \" \")) |&gt;\n  select(song_by)\n\n# A tibble: 10 × 1\n   song_by                                                               \n   &lt;chr&gt;                                                                 \n 1 Hear Me Now by Alok                                                   \n 2 Run the World (Girls) by Beyoncé                                      \n 3 Formation by Beyoncé                                                  \n 4 7/11 by Beyoncé                                                       \n 5 My Oh My (feat. DaBaby) by Camila Cabello                             \n 6 It's Automatic by Freestyle                                           \n 7 Poetic Justice by Kendrick Lamar                                      \n 8 A.D.H.D by Kendrick Lamar                                             \n 9 Ya Estuvo by Kid Frost                                                \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) by Mike WiLL Made-It",
    "crumbs": [
      "Strings: In-class Exercises (Part 1)"
    ]
  },
  {
    "objectID": "02_strings_part1.html#str_to_lower-str_to_upper-str_to_title",
    "href": "02_strings_part1.html#str_to_lower-str_to_upper-str_to_title",
    "title": "Strings: In-class Exercises (Part 1)",
    "section": "str_to_lower(), str_to_upper(), str_to_title()",
    "text": "str_to_lower(), str_to_upper(), str_to_title()\nThese are pretty self explanatory.\n\nspot_smaller |&gt;\n  select(title) |&gt;\n  mutate(title_to_lower = str_to_lower(title),\n         title_to_upper = str_to_upper(title))\n\n# A tibble: 10 × 3\n   title                                           title_to_lower title_to_upper\n   &lt;chr&gt;                                           &lt;chr&gt;          &lt;chr&gt;         \n 1 Hear Me Now                                     hear me now    HEAR ME NOW   \n 2 Run the World (Girls)                           run the world… RUN THE WORLD…\n 3 Formation                                       formation      FORMATION     \n 4 7/11                                            7/11           7/11          \n 5 My Oh My (feat. DaBaby)                         my oh my (fea… MY OH MY (FEA…\n 6 It's Automatic                                  it's automatic IT'S AUTOMATIC\n 7 Poetic Justice                                  poetic justice POETIC JUSTICE\n 8 A.D.H.D                                         a.d.h.d        A.D.H.D       \n 9 Ya Estuvo                                       ya estuvo      YA ESTUVO     \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Min… runnin (with … RUNNIN (WITH …\n\n# title is already in title case, so: \nstr_to_title(\"makes this into title case\")\n\n[1] \"Makes This Into Title Case\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 1)"
    ]
  },
  {
    "objectID": "02_strings_part1.html#matching-patterns",
    "href": "02_strings_part1.html#matching-patterns",
    "title": "Strings: In-class Exercises (Part 1)",
    "section": "Matching Patterns",
    "text": "Matching Patterns\nIn addition to manipulating strings, we might want to search through them to find matches. For example, can I find all the songs that start with M? The songs from 2016? The album titles that include a number?\n\nstr_view()\nThis function is helpful for viewing. It returns rows that contain the pattern you’re searching for, highlighting the pattern between &lt;.&gt; symbols and in a different color.\nThe first input is the vector, and the second input is the string/substring/pattern you are looking for.\n\nstr_view(spot_smaller$album_release_date, \"2016\")\n\n[1] │ &lt;2016&gt;-01-01\n[3] │ &lt;2016&gt;-04-23\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_view(spot_smaller$subgenre, \"pop\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n\nstr_view(spot_smaller$subgenre, \"hip hop\")\n\n[6] │ latin &lt;hip hop&gt;\n[7] │ &lt;hip hop&gt;\n[8] │ southern &lt;hip hop&gt;\n[9] │ latin &lt;hip hop&gt;\n\n\n\n\nstr_subset()\nstr_subset() takes a vector input and returns a (usually shorter) vector output. Compare the output from str_view() and str_subset() here. Both of these functions can be hard to work with in a tibble.\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_subset(spot_smaller$title, \"M\")\n\n[1] \"Hear Me Now\"                                      \n[2] \"My Oh My (feat. DaBaby)\"                          \n[3] \"Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)\"\n\n\n\n\nstr_detect()\nstr_detect takes a vector of strings (or single string) input and returns a vector of TRUE/FALSE (or single value). This makes it easy to work with in tibbles, using mutate or filter.\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_detect(spot_smaller$title, \"M\")\n\n [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE\n\nstr_detect(\"hello\", \"ll\")\n\n[1] TRUE\n\nspot_smaller |&gt; \n  select(title, album_name, artist) |&gt;\n  mutate(includes_M = str_detect(title, \"M\"))\n\n# A tibble: 10 × 4\n   title                                            album_name artist includes_M\n   &lt;chr&gt;                                            &lt;chr&gt;      &lt;chr&gt;  &lt;lgl&gt;     \n 1 Hear Me Now                                      Hear Me N… Alok   TRUE      \n 2 Run the World (Girls)                            4          Beyon… FALSE     \n 3 Formation                                        Lemonade   Beyon… FALSE     \n 4 7/11                                             BEYONCÉ [… Beyon… FALSE     \n 5 My Oh My (feat. DaBaby)                          Romance    Camil… TRUE      \n 6 It's Automatic                                   It's Auto… Frees… FALSE     \n 7 Poetic Justice                                   good kid,… Kendr… FALSE     \n 8 A.D.H.D                                          Section.80 Kendr… FALSE     \n 9 Ya Estuvo                                        Hispanic … Kid F… FALSE     \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Mina… Creed II:… Mike … TRUE      \n\nspot_smaller |&gt;  \n  select(title, album_name, artist) |&gt;\n  filter(str_detect(title, \"M\"))\n\n# A tibble: 3 × 3\n  title                                             album_name          artist  \n  &lt;chr&gt;                                             &lt;chr&gt;               &lt;chr&gt;   \n1 Hear Me Now                                       Hear Me Now         Alok    \n2 My Oh My (feat. DaBaby)                           Romance             Camila …\n3 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Creed II: The Album Mike Wi…\n\nspot_smaller |&gt; \n   select(title, album_name, artist, subgenre) |&gt;\n   filter(str_detect(subgenre, \"pop\"))\n\n# A tibble: 5 × 4\n  title                   album_name                 artist         subgenre    \n  &lt;chr&gt;                   &lt;chr&gt;                      &lt;chr&gt;          &lt;chr&gt;       \n1 Hear Me Now             Hear Me Now                Alok           indie popti…\n2 Run the World (Girls)   4                          Beyoncé        post-teen p…\n3 Formation               Lemonade                   Beyoncé        hip pop     \n4 7/11                    BEYONCÉ [Platinum Edition] Beyoncé        hip pop     \n5 My Oh My (feat. DaBaby) Romance                    Camila Cabello latin pop   \n\n\n\n\nstr_extract()\nstr_extract() takes a vector (or single) of strings input and returns a vector (or single) string output\n\nsingle_string\n\n[1] \"this is a string!\"\n\nstr_extract(single_string, \"this\")\n\n[1] \"this\"\n\n\nstr_extract() is more interesting when we want to identify a particular pattern to extract from the string.\nFor instance:\n\nstr_extract(\"find first vowel\", \"[aeiou]\")\n\n[1] \"i\"\n\nstr_extract(\"any numb3rs?\", \"\\\\d\")\n\n[1] \"3\"\n\nnumbers_here &lt;- c(\"numb3rs\", \"ar3\", \"h1d1ing\", \"almost\", \"ev3ryw4ere\")\n\nstr_extract(numbers_here, \"\\\\d\")\n\n[1] \"3\" \"3\" \"1\" NA  \"3\"\n\nstr_view(numbers_here, \"\\\\d\")\n\n[1] │ numb&lt;3&gt;rs\n[2] │ ar&lt;3&gt;\n[3] │ h&lt;1&gt;d&lt;1&gt;ing\n[5] │ ev&lt;3&gt;ryw&lt;4&gt;ere\n\n\nBecause str_extract returns a vector of the same length as its input, it also can be used within a tibble.\n\nspot_smaller |&gt;\n  select(title, artist, album_name) |&gt;\n  mutate(numbers = str_extract(album_name, \"\\\\d\"))\n\n# A tibble: 10 × 4\n   title                                             artist   album_name numbers\n   &lt;chr&gt;                                             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;  \n 1 Hear Me Now                                       Alok     Hear Me N… &lt;NA&gt;   \n 2 Run the World (Girls)                             Beyoncé  4          4      \n 3 Formation                                         Beyoncé  Lemonade   &lt;NA&gt;   \n 4 7/11                                              Beyoncé  BEYONCÉ [… &lt;NA&gt;   \n 5 My Oh My (feat. DaBaby)                           Camila … Romance    &lt;NA&gt;   \n 6 It's Automatic                                    Freesty… It's Auto… &lt;NA&gt;   \n 7 Poetic Justice                                    Kendric… good kid,… &lt;NA&gt;   \n 8 A.D.H.D                                           Kendric… Section.80 8      \n 9 Ya Estuvo                                         Kid Fro… Hispanic … &lt;NA&gt;   \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Mike Wi… Creed II:… &lt;NA&gt;   \n\n\nThe patterns we show here, “\\d” and “[aeiou]” are called regular expressions.",
    "crumbs": [
      "Strings: In-class Exercises (Part 1)"
    ]
  },
  {
    "objectID": "02_strings_part1.html#regular-expressions",
    "href": "02_strings_part1.html#regular-expressions",
    "title": "Strings: In-class Exercises (Part 1)",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nRegular expressions are a way to write general patterns… for instance the string “\\d” will find any digit (number). We can also specify whether we want the string to start or end with a certain letter.\nNotice the difference between the regular expression “M” and “^M”, “o” and “o$”\n\nstr_view(spot_smaller$title, \"M\")\n\n [1] │ Hear &lt;M&gt;e Now\n [5] │ &lt;M&gt;y Oh &lt;M&gt;y (feat. DaBaby)\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki &lt;M&gt;inaj)\n\nstr_view(spot_smaller$title, \"^M\")\n\n[5] │ &lt;M&gt;y Oh My (feat. DaBaby)\n\nstr_view(spot_smaller$title, \"o\")\n\n [1] │ Hear Me N&lt;o&gt;w\n [2] │ Run the W&lt;o&gt;rld (Girls)\n [3] │ F&lt;o&gt;rmati&lt;o&gt;n\n [6] │ It's Aut&lt;o&gt;matic\n [7] │ P&lt;o&gt;etic Justice\n [9] │ Ya Estuv&lt;o&gt;\n[10] │ Runnin (with A$AP R&lt;o&gt;cky, A$AP Ferg & Nicki Minaj)\n\nstr_view(spot_smaller$title, \"o$\")\n\n[9] │ Ya Estuv&lt;o&gt;\n\n\nBut how do I look for a dollar sign in my string? I use  to “escape” the special behavior of $. But  itself has special behavior… so I need two of them.\n\nstr_view(spot_smaller$title, \"\\\\$\")\n\n[10] │ Runnin (with A&lt;$&gt;AP Rocky, A&lt;$&gt;AP Ferg & Nicki Minaj)\n\n\n\nExample problem\nAre there any album names that contain numbers?\nstep 1: use str_view() to figure out an appropriate regular expression to use for searching.\n\nstr_view(spot_smaller$album_name, \"\\\\d\")\n\n[2] │ &lt;4&gt;\n[8] │ Section.&lt;8&gt;&lt;0&gt;\n\n\nstep 2: what kind of output do I want?\n\n# A list of the album names?\nstr_subset(spot_smaller$album_name, \"\\\\d\")\n\n[1] \"4\"          \"Section.80\"\n\n# A tibble? \nspot_smaller |&gt;\n  filter(str_detect(album_name, \"\\\\d\"))\n\n# A tibble: 2 × 6\n  title              artist album_release_date album_name subgenre playlist_name\n  &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n1 Run the World (Gi… Beyon… 2011-06-24         4          post-te… post-teen al…\n2 A.D.H.D            Kendr… 2011-07-02         Section.80 souther… Hip-Hop 'n R…",
    "crumbs": [
      "Strings: In-class Exercises (Part 1)"
    ]
  },
  {
    "objectID": "02_strings_part1.html#more-regular-expressions",
    "href": "02_strings_part1.html#more-regular-expressions",
    "title": "Strings: In-class Exercises (Part 1)",
    "section": "More regular expressions",
    "text": "More regular expressions\n[abc] - a, b, or c\n\nstr_view(spot_smaller$subgenre, \"[hp]op\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n[6] │ latin hip &lt;hop&gt;\n[7] │ hip &lt;hop&gt;\n[8] │ southern hip &lt;hop&gt;\n[9] │ latin hip &lt;hop&gt;\n\nstr_view(spot_smaller$subgenre, \"hip [hp]op\")\n\n[3] │ &lt;hip pop&gt;\n[4] │ &lt;hip pop&gt;\n[6] │ latin &lt;hip hop&gt;\n[7] │ &lt;hip hop&gt;\n[8] │ southern &lt;hip hop&gt;\n[9] │ latin &lt;hip hop&gt;\n\n\n[^abc] anything EXCEPT abc.\n\nstr_view(spot_smaller$album_name, \"[^\\\\d]\")\n\n [1] │ &lt;H&gt;&lt;e&gt;&lt;a&gt;&lt;r&gt;&lt; &gt;&lt;M&gt;&lt;e&gt;&lt; &gt;&lt;N&gt;&lt;o&gt;&lt;w&gt;\n [3] │ &lt;L&gt;&lt;e&gt;&lt;m&gt;&lt;o&gt;&lt;n&gt;&lt;a&gt;&lt;d&gt;&lt;e&gt;\n [4] │ &lt;B&gt;&lt;E&gt;&lt;Y&gt;&lt;O&gt;&lt;N&gt;&lt;C&gt;&lt;É&gt;&lt; &gt;&lt;[&gt;&lt;P&gt;&lt;l&gt;&lt;a&gt;&lt;t&gt;&lt;i&gt;&lt;n&gt;&lt;u&gt;&lt;m&gt;&lt; &gt;&lt;E&gt;&lt;d&gt;&lt;i&gt;&lt;t&gt;&lt;i&gt;&lt;o&gt;&lt;n&gt;&lt;]&gt;\n [5] │ &lt;R&gt;&lt;o&gt;&lt;m&gt;&lt;a&gt;&lt;n&gt;&lt;c&gt;&lt;e&gt;\n [6] │ &lt;I&gt;&lt;t&gt;&lt;'&gt;&lt;s&gt;&lt; &gt;&lt;A&gt;&lt;u&gt;&lt;t&gt;&lt;o&gt;&lt;m&gt;&lt;a&gt;&lt;t&gt;&lt;i&gt;&lt;c&gt;\n [7] │ &lt;g&gt;&lt;o&gt;&lt;o&gt;&lt;d&gt;&lt; &gt;&lt;k&gt;&lt;i&gt;&lt;d&gt;&lt;,&gt;&lt; &gt;&lt;m&gt;&lt;.&gt;&lt;A&gt;&lt;.&gt;&lt;A&gt;&lt;.&gt;&lt;d&gt;&lt; &gt;&lt;c&gt;&lt;i&gt;&lt;t&gt;&lt;y&gt;&lt; &gt;&lt;(&gt;&lt;D&gt;&lt;e&gt;&lt;l&gt;&lt;u&gt;&lt;x&gt;&lt;e&gt;&lt;)&gt;\n [8] │ &lt;S&gt;&lt;e&gt;&lt;c&gt;&lt;t&gt;&lt;i&gt;&lt;o&gt;&lt;n&gt;&lt;.&gt;80\n [9] │ &lt;H&gt;&lt;i&gt;&lt;s&gt;&lt;p&gt;&lt;a&gt;&lt;n&gt;&lt;i&gt;&lt;c&gt;&lt; &gt;&lt;C&gt;&lt;a&gt;&lt;u&gt;&lt;s&gt;&lt;i&gt;&lt;n&gt;&lt;g&gt;&lt; &gt;&lt;P&gt;&lt;a&gt;&lt;n&gt;&lt;i&gt;&lt;c&gt;\n[10] │ &lt;C&gt;&lt;r&gt;&lt;e&gt;&lt;e&gt;&lt;d&gt;&lt; &gt;&lt;I&gt;&lt;I&gt;&lt;:&gt;&lt; &gt;&lt;T&gt;&lt;h&gt;&lt;e&gt;&lt; &gt;&lt;A&gt;&lt;l&gt;&lt;b&gt;&lt;u&gt;&lt;m&gt;\n\nstr_view(spot_smaller$album_name, \"[^a-zA-Z ]\")\n\n [2] │ &lt;4&gt;\n [4] │ BEYONC&lt;É&gt; &lt;[&gt;Platinum Edition&lt;]&gt;\n [6] │ It&lt;'&gt;s Automatic\n [7] │ good kid&lt;,&gt; m&lt;.&gt;A&lt;.&gt;A&lt;.&gt;d city &lt;(&gt;Deluxe&lt;)&gt;\n [8] │ Section&lt;.&gt;&lt;8&gt;&lt;0&gt;\n[10] │ Creed II&lt;:&gt; The Album",
    "crumbs": [
      "Strings: In-class Exercises (Part 1)"
    ]
  },
  {
    "objectID": "02_strings_part1.html#bonus-content-not-in-the-pre-class-video",
    "href": "02_strings_part1.html#bonus-content-not-in-the-pre-class-video",
    "title": "Strings: In-class Exercises (Part 1)",
    "section": "Bonus content not in the pre-class video",
    "text": "Bonus content not in the pre-class video\n\nstr_glue()\nThis is a nice alternative to str_c(), where you only need a single set of quotes, and anything inside curly brackets {} is evaluated like it’s outside the quotes.\n\n# Thus, this code from earlier...\n\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"\n\nsong_count &lt;- spot_smaller |&gt; \n  count(artist) |&gt;\n  slice_max(n, n = 1)\nsong_count\n\n# A tibble: 1 × 2\n  artist      n\n  &lt;chr&gt;   &lt;int&gt;\n1 Beyoncé     3\n\nstr_c(\"The artist with the most songs in spot_smaller is\", song_count$artist, \"with\", song_count$n, \"songs.\", sep = \" \")\n\n[1] \"The artist with the most songs in spot_smaller is Beyoncé with 3 songs.\"\n\n# ... becomes this:\n\nsong_count |&gt; mutate(statement = str_glue(\"The artist with the most songs in spot_smaller is {artist} with {n} songs.\"))\n\n# A tibble: 1 × 3\n  artist      n statement                                                       \n  &lt;chr&gt;   &lt;int&gt; &lt;glue&gt;                                                          \n1 Beyoncé     3 The artist with the most songs in spot_smaller is Beyoncé with …\n\n# or \n\nstr_glue(\"The artist with the most songs in spot_smaller is {song_count$artist} with {song_count$n} songs.\")\n\nThe artist with the most songs in spot_smaller is Beyoncé with 3 songs.\n\n\nstr_glue() can also be applied to an entire column vector:\n\nspot_smaller |&gt;\n  mutate(statement = str_glue(\"{artist} released {album_name} on {album_release_date}.\")) |&gt;\n  select(statement)\n\n# A tibble: 10 × 1\n   statement                                                       \n   &lt;glue&gt;                                                          \n 1 Alok released Hear Me Now on 2016-01-01.                        \n 2 Beyoncé released 4 on 2011-06-24.                               \n 3 Beyoncé released Lemonade on 2016-04-23.                        \n 4 Beyoncé released BEYONCÉ [Platinum Edition] on 2014-11-24.      \n 5 Camila Cabello released Romance on 2019-12-06.                  \n 6 Freestyle released It's Automatic on 2013-11-28.                \n 7 Kendrick Lamar released good kid, m.A.A.d city (Deluxe) on 2012.\n 8 Kendrick Lamar released Section.80 on 2011-07-02.               \n 9 Kid Frost released Hispanic Causing Panic on 1990-01-01.        \n10 Mike WiLL Made-It released Creed II: The Album on 2018-11-16.   \n\n\nAnd if you wanted to include {} in your statement, you can double up {} to serve as an escape character:\n\nspot_smaller |&gt;\n  mutate(statement = str_glue(\"{artist} released {album_name} on {album_release_date} {{according to Spotify}}.\")) |&gt;\n  select(statement)\n\n# A tibble: 10 × 1\n   statement                                                                    \n   &lt;glue&gt;                                                                       \n 1 Alok released Hear Me Now on 2016-01-01 {according to Spotify}.              \n 2 Beyoncé released 4 on 2011-06-24 {according to Spotify}.                     \n 3 Beyoncé released Lemonade on 2016-04-23 {according to Spotify}.              \n 4 Beyoncé released BEYONCÉ [Platinum Edition] on 2014-11-24 {according to Spot…\n 5 Camila Cabello released Romance on 2019-12-06 {according to Spotify}.        \n 6 Freestyle released It's Automatic on 2013-11-28 {according to Spotify}.      \n 7 Kendrick Lamar released good kid, m.A.A.d city (Deluxe) on 2012 {according t…\n 8 Kendrick Lamar released Section.80 on 2011-07-02 {according to Spotify}.     \n 9 Kid Frost released Hispanic Causing Panic on 1990-01-01 {according to Spotif…\n10 Mike WiLL Made-It released Creed II: The Album on 2018-11-16 {according to S…\n\n\n\n\nseparate_wider_delim() and its cousins\nWhen multiple variables are crammed together into a single string, the separate_ functions can be used to extract the pieces are produce additional rows (longer) or columns (wider). We show one such example below, using the optional “too_few” setting to diagnose issues after getting a warning message the first time.\n\nspot_smaller |&gt;\n  separate_wider_delim(\n    album_release_date,\n    delim = \"-\",\n    names = c(\"year\", \"month\", \"day\"),\n    too_few = \"debug\"\n  ) |&gt;\n  print(width = Inf)\n\nWarning: Debug mode activated: adding variables `album_release_date_ok`,\n`album_release_date_pieces`, and `album_release_date_remainder`.\n\n\n# A tibble: 10 × 12\n   title                                             artist            year \n   &lt;chr&gt;                                             &lt;chr&gt;             &lt;chr&gt;\n 1 Hear Me Now                                       Alok              2016 \n 2 Run the World (Girls)                             Beyoncé           2011 \n 3 Formation                                         Beyoncé           2016 \n 4 7/11                                              Beyoncé           2014 \n 5 My Oh My (feat. DaBaby)                           Camila Cabello    2019 \n 6 It's Automatic                                    Freestyle         2013 \n 7 Poetic Justice                                    Kendrick Lamar    2012 \n 8 A.D.H.D                                           Kendrick Lamar    2011 \n 9 Ya Estuvo                                         Kid Frost         1990 \n10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Mike WiLL Made-It 2018 \n   month day   album_release_date album_release_date_ok\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;              &lt;lgl&gt;                \n 1 01    01    2016-01-01         TRUE                 \n 2 06    24    2011-06-24         TRUE                 \n 3 04    23    2016-04-23         TRUE                 \n 4 11    24    2014-11-24         TRUE                 \n 5 12    06    2019-12-06         TRUE                 \n 6 11    28    2013-11-28         TRUE                 \n 7 &lt;NA&gt;  &lt;NA&gt;  2012               FALSE                \n 8 07    02    2011-07-02         TRUE                 \n 9 01    01    1990-01-01         TRUE                 \n10 11    16    2018-11-16         TRUE                 \n   album_release_date_pieces album_release_date_remainder\n                       &lt;int&gt; &lt;chr&gt;                       \n 1                         3 \"\"                          \n 2                         3 \"\"                          \n 3                         3 \"\"                          \n 4                         3 \"\"                          \n 5                         3 \"\"                          \n 6                         3 \"\"                          \n 7                         1 \"\"                          \n 8                         3 \"\"                          \n 9                         3 \"\"                          \n10                         3 \"\"                          \n   album_name                      subgenre        \n   &lt;chr&gt;                           &lt;chr&gt;           \n 1 Hear Me Now                     indie poptimism \n 2 4                               post-teen pop   \n 3 Lemonade                        hip pop         \n 4 BEYONCÉ [Platinum Edition]      hip pop         \n 5 Romance                         latin pop       \n 6 It's Automatic                  latin hip hop   \n 7 good kid, m.A.A.d city (Deluxe) hip hop         \n 8 Section.80                      southern hip hop\n 9 Hispanic Causing Panic          latin hip hop   \n10 Creed II: The Album             gangster rap    \n   playlist_name                                                              \n   &lt;chr&gt;                                                                      \n 1 \"Chillout & Remixes \\U0001f49c\"                                            \n 2 \"post-teen alternative, indie, pop (large variety)\"                        \n 3 \"Feeling Accomplished\"                                                     \n 4 \"Feeling Accomplished\"                                                     \n 5 \"2020 Hits & 2019  Hits – Top Global Tracks \\U0001f525\\U0001f525\\U0001f525\"\n 6 \"80's Freestyle/Disco Dance Party (Set Crossfade to 4-Seconds)\"            \n 7 \"Hip Hop Controller\"                                                       \n 8 \"Hip-Hop 'n RnB\"                                                           \n 9 \"HIP-HOP: Latin Rap ['89-present]\"                                         \n10 \"RAP Gangsta\"                                                              \n\nspot_smaller |&gt;\n  separate_wider_delim(\n    album_release_date,\n    delim = \"-\",\n    names = c(\"year\", \"month\", \"day\"),\n    too_few = \"align_start\"\n  )\n\n# A tibble: 10 × 8\n   title              artist year  month day   album_name subgenre playlist_name\n   &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now        Alok   2016  01    01    Hear Me N… indie p… \"Chillout & …\n 2 Run the World (Gi… Beyon… 2011  06    24    4          post-te… \"post-teen a…\n 3 Formation          Beyon… 2016  04    23    Lemonade   hip pop  \"Feeling Acc…\n 4 7/11               Beyon… 2014  11    24    BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. D… Camil… 2019  12    06    Romance    latin p… \"2020 Hits &…\n 6 It's Automatic     Frees… 2013  11    28    It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice     Kendr… 2012  &lt;NA&gt;  &lt;NA&gt;  good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D            Kendr… 2011  07    02    Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo          Kid F… 1990  01    01    Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$AP… Mike … 2018  11    16    Creed II:… gangste… \"RAP Gangsta\"\n\n\nIf there is a definable pattern, but the pattern is a bit weird, we can often use separate_wider_regex() to extract the correct values and build a tidy data set:\n\ndf &lt;- tribble(\n  ~str,\n  \"&lt;Sheryl&gt;-F_34\",\n  \"&lt;Kisha&gt;-F_45\", \n  \"&lt;Brandon&gt;-N_33\",\n  \"&lt;Sharon&gt;-F_38\", \n  \"&lt;Penny&gt;-F_58\",\n  \"&lt;Justin&gt;-M_41\", \n  \"&lt;Patricia&gt;-F_84\", \n)\n\ndf |&gt; \n  separate_wider_regex(\n    str,\n    patterns = c(\n      \"&lt;\", \n      name = \"[A-Za-z]+\", \n      \"&gt;-\", \n      gender = \".\",\n      \"_\",\n      age = \"[0-9]+\"\n    )\n  )\n\n# A tibble: 7 × 3\n  name     gender age  \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;\n1 Sheryl   F      34   \n2 Kisha    F      45   \n3 Brandon  N      33   \n4 Sharon   F      38   \n5 Penny    F      58   \n6 Justin   M      41   \n7 Patricia F      84",
    "crumbs": [
      "Strings: In-class Exercises (Part 1)"
    ]
  },
  {
    "objectID": "01_review164.html",
    "href": "01_review164.html",
    "title": "Review of Data Science 1",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\n\nDeterminants of COVID vaccination rates\nFirst, a little detour to describe several alternatives for reading in data:\nIf you navigate to my Github account, and find the 264_fall_2025 repo, there is a Data folder inside. You can then click on vacc_Mar21.csv to see the data we want to download. This link should also get you there, but it’s good to be able to navigate there yourself.\n\n# Approach 1\n1vaccine_data &lt;- read_csv(\"Data/vaccinations_2021.csv\")\n\n# Approach 2\n2vaccine_data &lt;- read_csv(\"~/264_fall_2025/Data/vaccinations_2021.csv\")\n\n# Approach 3\n3vaccine_data &lt;- read_csv(\"https://proback.github.io/264_fall_2025/Data/vaccinations_2021.csv\")\n\n# Approach 4\n4vaccine_data &lt;- read_csv(\"https://raw.githubusercontent.com/proback/264_fall_2025/refs/heads/main/Data/vaccinations_2021.csv\")\n\n\n1\n\nApproach 1: create a Data folder in the same location where this .qmd file resides, and then store vaccinations_2021.csv in that Data folder\n\n2\n\nApproach 2: give R the complete path to the location of vaccinations_2021.csv, starting with Home (~)\n\n3\n\nApproach 3: link to our course webpage, and then know we have a Data folder containing all our csvs\n\n4\n\nApproach 4: navigate to the data in GitHub, hit the Raw button, and copy that link\n\n\n\n\nA recent Stat 272 project examined determinants of covid vaccination rates at the county level. Our data set contains 3053 rows (1 for each county in the US) and 14 columns; here is a quick description of the variables we’ll be using:\n\nstate = state the county is located in\ncounty = name of the county\nregion = region the state is located in\nmetro_status = Is the county considered “Metro” or “Non-metro”?\nrural_urban_code = from 1 (most urban) to 9 (most rural)\nperc_complete_vac = percent of county completely vaccinated as of 11/9/21\ntot_pop = total population in the county\nvotes_Trump = number of votes for Trump in the county in 2020\nvotes_Biden = number of votes for Biden in the county in 2020\nperc_Biden = percent of votes for Biden in the county in 2020\ned_somecol_perc = percent with some education beyond high school (but not a Bachelor’s degree)\ned_bachormore_perc = percent with a Bachelor’s degree or more\nunemployment_rate_2020 = county unemployment rate in 2020\nmedian_HHincome_2019 = county’s median household income in 2019\n\n\nConsider only Minnesota and its surrounding states (Iowa, Wisconsin, North Dakota, and South Dakota). We want to examine the relationship between the percentage who voted for Biden and the percentage of complete vaccinations by state. Generate two plots to examine this relationship:\n\n\nA scatterplot with points and smoothers colored by state. Make sure the legend is ordered in a meaningful way, and include good labels on your axes and your legend. Also leave off the error bars from your smoothers.\nOne plot per state containing a scatterplot and a smoother.\n\nDescribe which plot you prefer and why. What can you learn from your preferred plot?\n\nWe wish to compare the proportions of counties in each region with median household income above the national median ($69,560).\n\n\nFill in the blanks below to produce a segmented bar plot with regions ordered from highest proportion above the median to lowest.\nCreate a table of proportions by region to illustrate that your bar plot in (a) is in the correct order (you should find two regions that are really close when you just try to eyeball differences).\nExplain why we can replace fct_relevel(region, FILL IN CODE) with\n\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560, .fun = mean))\nbut not\nmutate(region_sort = fct_reorder(region, median_HHincome_2019 &lt; 69560))\n\nvaccine_data |&gt;\n  mutate(HHincome_vs_national = ifelse(median_HHincome_2019 &lt; 69560, FILL IN CODE)) |&gt;\n  mutate(region_sort = fct_relevel(region, FILL IN CODE)) |&gt;\n  ggplot(mapping = aes(x = region_sort, fill = HHincome_vs_national)) +\n    geom_bar(position = \"fill\")\n\n\nWe want to examine the distribution of total county populations and then see how it’s related to vaccination rates.\n\n\nCarefully and thoroughly explain why the two histograms below provide different plots.\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop / 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\nvaccine_data |&gt;\n  mutate(tot_pop_millions = tot_pop %/% 1000000) |&gt;\n  ggplot(mapping = aes(x = tot_pop_millions)) +\n    geom_histogram(bins = 40) +\n    labs(x = \"Total population in millions\")\n\n\n\n\n\n\n\n\n\nFind the top 5 counties in terms of total population.\nPlot a histogram of logged population and describe this distribution.\nPlot the relationship between log population and percent vaccinated using separate colors for Metro and Non-metro counties (be sure there’s no 3rd color used for NAs). Reduce the size and transparency of each point to make the plot more readable. Describe what you can learn from this plot.\n\n\nProduce 3 different plots for illustrating the relationship between the rural_urban_code and percent vaccinated. Hint: you can sometimes turn numeric variables into categorical variables for plotting purposes (e.g. as.factor(), ifelse()).\n\nState your favorite plot, why you like it better than the other two, and what you can learn from your favorite plot. Create an alt text description of your favorite plot, using the Four Ingredient Model. See this link for reminders and references about alt text.\n\nBEFORE running the code below, sketch the plot that will be produced by R. AFTER running the code, describe what conclusion(s) can we draw from this plot?\n\n\nvaccine_data |&gt;\n  filter(!is.na(perc_Biden)) |&gt;\n  mutate(big_states = fct_lump(state, n = 10)) |&gt;\n  group_by(big_states) |&gt;\n  summarize(IQR_Biden = IQR(perc_Biden)) |&gt;\n  mutate(big_states = fct_reorder(big_states, IQR_Biden)) |&gt;\n  ggplot() + \n    geom_point(aes(x = IQR_Biden, y = big_states))\n\n\nIn this question we will focus only on the 12 states in the Midwest (i.e. where region == “Midwest”).\n\n\nCreate a tibble with the following information for each state. Order states from least to greatest state population.\n\n\nnumber of different rural_urban_codes represented among the state’s counties (there are 9 possible)\ntotal state population\nproportion of Metro counties\nmedian unemployment rate\n\n\nUse your tibble in (a) to produce a plot of the relationship between proportion of Metro counties and median unemployment rate. Points should be colored by the number of different rural_urban_codes in a state, but a single linear trend should be fit to all points. What can you conclude from the plot?\n\n\nGenerate an appropriate plot to compare vaccination rates between two subregions of the US: New England (which contains the states Maine, Vermont, New Hampshire, Massachusetts, Connecticut, Rhode Island) and the Upper Midwest (which, according to the USGS, contains the states Minnesota, Wisconsin, Michigan, Illinois, Indiana, and Iowa). What can you conclude from your plot?\n\nIn this next section, we consider a few variables that could have been included in our data set, but were NOT. Thus, you won’t be able to write and test code, but you nevertheless should be able to use your knowledge of the tidyverse to answer these questions.\nHere are the hypothetical variables:\n\nHR_party = party of that county’s US Representative (Republican, Democrat, Independent, Green, or Libertarian)\npeople_per_MD = number of residents per doctor (higher values = fewer doctors)\nperc_over_65 = percent of residents over 65 years old\nperc_white = percent of residents who identify as white\n\n\nHypothetical R chunk #1:\n\n\n# Hypothetical R chunk 1\ntemp &lt;- vaccine_data |&gt;\n  mutate(new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac),\n         MD_group = cut_number(people_per_MD, 3)) |&gt;\n  group_by(MD_group) |&gt;\n  summarise(n = n(),\n            mean_perc_vac = mean(new_perc_vac, na.rm = TRUE),\n            mean_white = mean(perc_white, na.rm = TRUE))\n\n\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced new_perc_vac = ifelse(perc_complete_vac &gt; 95, NA, perc_complete_vac) with new_perc_vac = ifelse(perc_complete_vac &gt; 95, perc_complete_vac, NA)?\nWhat would happen if we replaced mean_white = mean(perc_white, na.rm = TRUE) with mean_white = mean(perc_white)?\nWhat would happen if we removed group_by(MD_group)?\n\n\nHypothetical R chunk #2:\n\n\n# Hypothetical R chunk 2\nggplot(data = vaccine_data) +\n  geom_point(mapping = aes(x = perc_over_65, y = perc_complete_vac, \n                           color = HR_party)) +\n  geom_smooth()\n\ntemp &lt;- vaccine_data |&gt;\n  group_by(HR_party) |&gt;\n  summarise(var1 = n()) |&gt;\n  arrange(desc(var1)) |&gt;\n  slice_head(n = 3)\n\nvaccine_data |&gt;\n  ggplot(mapping = aes(x = fct_reorder(HR_party, perc_over_65, .fun = median), \n                       y = perc_over_65)) +\n    geom_boxplot()\n\n\nWhy would the first plot produce an error?\nDescribe the tibble temp created above. What would be the dimensions? What do rows and columns represent?\nWhat would happen if we replaced fct_reorder(HR_party, perc_over_65, .fun = median) with HR_party?\n\n\nHypothetical R chunk #3:\n\n\n# Hypothetical R chunk 3\nvaccine_data |&gt;\n  filter(!is.na(people_per_MD)) |&gt;\n  mutate(state_lump = fct_lump(state, n = 4)) |&gt;\n  group_by(state_lump, rural_urban_code) |&gt;\n  summarise(mean_people_per_MD = mean(people_per_MD)) |&gt;\n  ggplot(mapping = aes(x = rural_urban_code, y = mean_people_per_MD, \n      colour = fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD))) +\n    geom_line()\n\n\nDescribe the tibble piped into the ggplot above. What would be the dimensions? What do rows and columns represent?\nCarefully describe the plot created above.\nWhat would happen if we removed filter(!is.na(people_per_MD))?\nWhat would happen if we replaced fct_reorder2(state_lump, rural_urban_code, mean_people_per_MD) with state_lump?",
    "crumbs": [
      "Review of Data Science 1"
    ]
  },
  {
    "objectID": "03_strings_part2.html",
    "href": "03_strings_part2.html",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nThis uses parts of R4DS Ch 14: Strings and Ch 15: Regular Expressions (both the first and second editions).",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "03_strings_part2.html#manipulating-strings",
    "href": "03_strings_part2.html#manipulating-strings",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Manipulating strings",
    "text": "Manipulating strings\nstr functions to know for manipulating strings:\n\nstr_length()\nstr_sub()\nstr_c()\nstr_to_lower()\nstr_to_upper()\nstr_to_title()\nstr_replace() not in 02_strings examples\n\n\nlibrary(tidyverse)\n\n#spotify &lt;- read_csv(\"Data/spotify.csv\") \nspotify &lt;- read_csv(\"https://proback.github.io/264_fall_2025/Data/spotify.csv\")\n\nspot_smaller &lt;- spotify |&gt;\n  select(\n    title, \n    artist, \n    album_release_date, \n    album_name, \n    subgenre, \n    playlist_name\n  )\n\nspot_smaller &lt;- spot_smaller[c(5, 32, 49, 52, 83, 175, 219, 231, 246, 265), ]\nspot_smaller\n\n# A tibble: 10 × 6\n   title             artist album_release_date album_name subgenre playlist_name\n   &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        \n 1 Hear Me Now       Alok   2016-01-01         Hear Me N… indie p… \"Chillout & …\n 2 Run the World (G… Beyon… 2011-06-24         4          post-te… \"post-teen a…\n 3 Formation         Beyon… 2016-04-23         Lemonade   hip pop  \"Feeling Acc…\n 4 7/11              Beyon… 2014-11-24         BEYONCÉ [… hip pop  \"Feeling Acc…\n 5 My Oh My (feat. … Camil… 2019-12-06         Romance    latin p… \"2020 Hits &…\n 6 It's Automatic    Frees… 2013-11-28         It's Auto… latin h… \"80's Freest…\n 7 Poetic Justice    Kendr… 2012               good kid,… hip hop  \"Hip Hop Con…\n 8 A.D.H.D           Kendr… 2011-07-02         Section.80 souther… \"Hip-Hop 'n …\n 9 Ya Estuvo         Kid F… 1990-01-01         Hispanic … latin h… \"HIP-HOP: La…\n10 Runnin (with A$A… Mike … 2018-11-16         Creed II:… gangste… \"RAP Gangsta\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "03_strings_part2.html#warm-up",
    "href": "03_strings_part2.html#warm-up",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Warm-up",
    "text": "Warm-up\n\nDescribe what EACH of the str_ functions below does. Then, create a new variable “month” which is the two digit month from album_release_date\n\n\nspot_new &lt;- spot_smaller |&gt;\n  select(title, album_release_date) |&gt;\n  mutate(title_length = str_length(title),\n         year = str_sub(album_release_date, 1, 4),\n         title_lower = str_to_lower(title),\n         album_release_date2 = str_replace_all(album_release_date, \"-\", \"/\"))\nspot_new\n\n# A tibble: 10 × 6\n   title   album_release_date title_length year  title_lower album_release_date2\n   &lt;chr&gt;   &lt;chr&gt;                     &lt;int&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;              \n 1 Hear M… 2016-01-01                   11 2016  hear me now 2016/01/01         \n 2 Run th… 2011-06-24                   21 2011  run the wo… 2011/06/24         \n 3 Format… 2016-04-23                    9 2016  formation   2016/04/23         \n 4 7/11    2014-11-24                    4 2014  7/11        2014/11/24         \n 5 My Oh … 2019-12-06                   23 2019  my oh my (… 2019/12/06         \n 6 It's A… 2013-11-28                   14 2013  it's autom… 2013/11/28         \n 7 Poetic… 2012                         14 2012  poetic jus… 2012               \n 8 A.D.H.D 2011-07-02                    7 2011  a.d.h.d     2011/07/02         \n 9 Ya Est… 1990-01-01                    9 1990  ya estuvo   1990/01/01         \n10 Runnin… 2018-11-16                   49 2018  runnin (wi… 2018/11/16         \n\nmax_length &lt;- max(spot_new$title_length)\n\nstr_c(\"The longest title is\", max_length, \"characters long.\", sep = \" \")\n\n[1] \"The longest title is 49 characters long.\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "03_strings_part2.html#important-functions-for-identifying-strings-which-match",
    "href": "03_strings_part2.html#important-functions-for-identifying-strings-which-match",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Important functions for identifying strings which match",
    "text": "Important functions for identifying strings which match\nstr_view() : most useful for testing str_subset() : useful for printing matches to the console str_detect() : useful when working within a tibble\n\nIdentify the input type and output type for each of these examples:\n\n\nstr_view(spot_smaller$subgenre, \"pop\")\n\n[1] │ indie &lt;pop&gt;timism\n[2] │ post-teen &lt;pop&gt;\n[3] │ hip &lt;pop&gt;\n[4] │ hip &lt;pop&gt;\n[5] │ latin &lt;pop&gt;\n\ntypeof(str_view(spot_smaller$subgenre, \"pop\"))\n\n[1] \"character\"\n\nclass(str_view(spot_smaller$subgenre, \"pop\"))\n\n[1] \"stringr_view\"\n\nstr_view(spot_smaller$subgenre, \"pop\", match = NA)\n\n [1] │ indie &lt;pop&gt;timism\n [2] │ post-teen &lt;pop&gt;\n [3] │ hip &lt;pop&gt;\n [4] │ hip &lt;pop&gt;\n [5] │ latin &lt;pop&gt;\n [6] │ latin hip hop\n [7] │ hip hop\n [8] │ southern hip hop\n [9] │ latin hip hop\n[10] │ gangster rap\n\nstr_view(spot_smaller$subgenre, \"pop\", html = TRUE)\n\n\n\n\nstr_subset(spot_smaller$subgenre, \"pop\")\n\n[1] \"indie poptimism\" \"post-teen pop\"   \"hip pop\"         \"hip pop\"        \n[5] \"latin pop\"      \n\nstr_detect(spot_smaller$subgenre, \"pop\")\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\n\nUse str_detect to print the rows of the spot_smaller tibble containing songs that have “pop” in the subgenre. (i.e. make a new tibble with fewer rows)\nFind the mean song title length for songs with “pop” in the subgenre and songs without “pop” in the subgenre.\n\nProducing a table like this would be great:",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "03_strings_part2.html#matching-patterns-with-regular-expressions",
    "href": "03_strings_part2.html#matching-patterns-with-regular-expressions",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Matching patterns with regular expressions",
    "text": "Matching patterns with regular expressions\n^abc string starts with abc abc$ string ends with abc . any character [abc] a or b or c [^abc] anything EXCEPT a or b or c\n\n# Guess the output!\n\nstr_view(spot_smaller$artist, \"^K\")\n\n[7] │ &lt;K&gt;endrick Lamar\n[8] │ &lt;K&gt;endrick Lamar\n[9] │ &lt;K&gt;id Frost\n\nstr_view(spot_smaller$album_release_date, \"01$\")\n\n[1] │ 2016-01-&lt;01&gt;\n[9] │ 1990-01-&lt;01&gt;\n\nstr_view(spot_smaller$title, \"^.. \")\n\n[5] │ &lt;My &gt;Oh My (feat. DaBaby)\n[9] │ &lt;Ya &gt;Estuvo\n\nstr_view(spot_smaller$artist, \"[^A-Za-z ]\")\n\n [2] │ Beyonc&lt;é&gt;\n [3] │ Beyonc&lt;é&gt;\n [4] │ Beyonc&lt;é&gt;\n[10] │ Mike WiLL Made&lt;-&gt;It\n\n\n\nGiven the corpus of common words in stringr::words, create regular expressions that find all words that:\n\n\nStart with “y”.\nEnd with “x”\nAre exactly three letters long.\nHave seven letters or more.\nStart with a vowel.\nEnd with ed, but not with eed.\nWords where q is not followed by u. (are there any in words?)\n\n\n# Try using str_view() or str_subset()\n\n# For example, to find words with \"tion\" at any point, I could use:\nstr_view(words, \"tion\")\n\n[181] │ condi&lt;tion&gt;\n[347] │ func&lt;tion&gt;\n[516] │ men&lt;tion&gt;\n[536] │ mo&lt;tion&gt;\n[543] │ na&lt;tion&gt;\n[631] │ posi&lt;tion&gt;\n[667] │ ques&lt;tion&gt;\n[695] │ rela&lt;tion&gt;\n[732] │ sec&lt;tion&gt;\n[804] │ sta&lt;tion&gt;\n\nstr_subset(words, \"tion\")\n\n [1] \"condition\" \"function\"  \"mention\"   \"motion\"    \"nation\"    \"position\" \n [7] \"question\"  \"relation\"  \"section\"   \"station\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "03_strings_part2.html#more-useful-regular-expressions",
    "href": "03_strings_part2.html#more-useful-regular-expressions",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "More useful regular expressions:",
    "text": "More useful regular expressions:\n\\d - any number \\s - any space, tab, etc \\b - any boundary: space, ., etc.\n\nstr_view(spot_smaller$album_name, \"\\\\d\")\n\n[2] │ &lt;4&gt;\n[8] │ Section.&lt;8&gt;&lt;0&gt;\n\nstr_view(spot_smaller$album_name, \"\\\\s\")\n\n [1] │ Hear&lt; &gt;Me&lt; &gt;Now\n [4] │ BEYONCÉ&lt; &gt;[Platinum&lt; &gt;Edition]\n [6] │ It's&lt; &gt;Automatic\n [7] │ good&lt; &gt;kid,&lt; &gt;m.A.A.d&lt; &gt;city&lt; &gt;(Deluxe)\n [9] │ Hispanic&lt; &gt;Causing&lt; &gt;Panic\n[10] │ Creed&lt; &gt;II:&lt; &gt;The&lt; &gt;Album\n\nstr_view(spot_smaller$album_name, \"\\\\b\")\n\n [1] │ &lt;&gt;Hear&lt;&gt; &lt;&gt;Me&lt;&gt; &lt;&gt;Now&lt;&gt;\n [2] │ &lt;&gt;4&lt;&gt;\n [3] │ &lt;&gt;Lemonade&lt;&gt;\n [4] │ &lt;&gt;BEYONCÉ&lt;&gt; [&lt;&gt;Platinum&lt;&gt; &lt;&gt;Edition&lt;&gt;]\n [5] │ &lt;&gt;Romance&lt;&gt;\n [6] │ &lt;&gt;It&lt;&gt;'&lt;&gt;s&lt;&gt; &lt;&gt;Automatic&lt;&gt;\n [7] │ &lt;&gt;good&lt;&gt; &lt;&gt;kid&lt;&gt;, &lt;&gt;m&lt;&gt;.&lt;&gt;A&lt;&gt;.&lt;&gt;A&lt;&gt;.&lt;&gt;d&lt;&gt; &lt;&gt;city&lt;&gt; (&lt;&gt;Deluxe&lt;&gt;)\n [8] │ &lt;&gt;Section&lt;&gt;.&lt;&gt;80&lt;&gt;\n [9] │ &lt;&gt;Hispanic&lt;&gt; &lt;&gt;Causing&lt;&gt; &lt;&gt;Panic&lt;&gt;\n[10] │ &lt;&gt;Creed&lt;&gt; &lt;&gt;II&lt;&gt;: &lt;&gt;The&lt;&gt; &lt;&gt;Album&lt;&gt;\n\n\nHere are the regular expression special characters that require an escape character (a preceding  ):  ^ $ . ? * | + ( ) [ {\nFor any characters with special properties, use  to “escape” its special meaning … but  is itself a special character … so we need two \\! (e.g. \\$, \\., etc.)\n\nstr_view(spot_smaller$title, \"$\")\n\n [1] │ Hear Me Now&lt;&gt;\n [2] │ Run the World (Girls)&lt;&gt;\n [3] │ Formation&lt;&gt;\n [4] │ 7/11&lt;&gt;\n [5] │ My Oh My (feat. DaBaby)&lt;&gt;\n [6] │ It's Automatic&lt;&gt;\n [7] │ Poetic Justice&lt;&gt;\n [8] │ A.D.H.D&lt;&gt;\n [9] │ Ya Estuvo&lt;&gt;\n[10] │ Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj)&lt;&gt;\n\nstr_view(spot_smaller$title, \"\\\\$\")\n\n[10] │ Runnin (with A&lt;$&gt;AP Rocky, A&lt;$&gt;AP Ferg & Nicki Minaj)\n\n\n\nIn bigspotify, how many track_names include a $? Be sure you print the track_names you find and make sure the dollar sign is not just in a featured artist!\nIn bigspotify, how many track_names include a dollar amount (a $ followed by a number).",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "03_strings_part2.html#repetition",
    "href": "03_strings_part2.html#repetition",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Repetition",
    "text": "Repetition\n? 0 or 1 times + 1 or more * 0 or more {n} exactly n times {n,} n or more times {,m} at most m times {n,m} between n and m times\n\nstr_view(spot_smaller$album_name, \"[A-Z]{2,}\")\n\n [4] │ &lt;BEYONC&gt;É [Platinum Edition]\n[10] │ Creed &lt;II&gt;: The Album\n\nstr_view(spot_smaller$album_release_date, \"\\\\d{4}-\\\\d{2}\")\n\n [1] │ &lt;2016-01&gt;-01\n [2] │ &lt;2011-06&gt;-24\n [3] │ &lt;2016-04&gt;-23\n [4] │ &lt;2014-11&gt;-24\n [5] │ &lt;2019-12&gt;-06\n [6] │ &lt;2013-11&gt;-28\n [8] │ &lt;2011-07&gt;-02\n [9] │ &lt;1990-01&gt;-01\n[10] │ &lt;2018-11&gt;-16\n\n\nUse at least 1 repetition symbol when solving 8-10 below\n\nModify the first regular expression above to also pick up “A.A” (in addition to “BEYONC” and “II”). That is, pick up strings where there might be a period between capital letters.\nCreate some strings that satisfy these regular expressions and explain.\n\n\n“^.*$”\n“\\{.+\\}”\n\n\nCreate regular expressions to find all stringr::words that:\n\n\nStart with three consonants.\nHave two or more vowel-consonant pairs in a row.",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "03_strings_part2.html#useful-functions-for-handling-patterns",
    "href": "03_strings_part2.html#useful-functions-for-handling-patterns",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Useful functions for handling patterns",
    "text": "Useful functions for handling patterns\nstr_extract() : extract a string that matches a pattern str_count() : count how many times a pattern occurs within a string\n\nstr_extract(spot_smaller$album_release_date, \"\\\\d{4}-\\\\d{2}\")\n\n [1] \"2016-01\" \"2011-06\" \"2016-04\" \"2014-11\" \"2019-12\" \"2013-11\" NA       \n [8] \"2011-07\" \"1990-01\" \"2018-11\"\n\nspot_smaller |&gt;\n  select(album_release_date) |&gt;\n  mutate(year_month = str_extract(album_release_date, \"\\\\d{4}-\\\\d{2}\"))\n\n# A tibble: 10 × 2\n   album_release_date year_month\n   &lt;chr&gt;              &lt;chr&gt;     \n 1 2016-01-01         2016-01   \n 2 2011-06-24         2011-06   \n 3 2016-04-23         2016-04   \n 4 2014-11-24         2014-11   \n 5 2019-12-06         2019-12   \n 6 2013-11-28         2013-11   \n 7 2012               &lt;NA&gt;      \n 8 2011-07-02         2011-07   \n 9 1990-01-01         1990-01   \n10 2018-11-16         2018-11   \n\nspot_smaller |&gt;\n  select(artist) |&gt;\n  mutate(n_vowels = str_count(artist, \"[aeiou]\"))\n\n# A tibble: 10 × 2\n   artist            n_vowels\n   &lt;chr&gt;                &lt;int&gt;\n 1 Alok                     1\n 2 Beyoncé                  2\n 3 Beyoncé                  2\n 4 Beyoncé                  2\n 5 Camila Cabello           6\n 6 Freestyle                3\n 7 Kendrick Lamar           4\n 8 Kendrick Lamar           4\n 9 Kid Frost                2\n10 Mike WiLL Made-It        5\n\n\n\nIn the spot_smaller dataset, how many words are in each title? (hint \\b)\nIn the spot_smaller dataset, extract the first word from every title. Show how you would print out these words as a vector and how you would create a new column on the spot_smaller tibble. That is, produce this:\n\n\n# [1] \"Hear\"      \"Run\"       \"Formation\" \"7/11\"      \"My\"        \"It's\"     \n# [7] \"Poetic\"    \"A.D.H.D\"   \"Ya\"        \"Runnin\"   \n\nThen this:\n\n# A tibble: 10 × 2\n#   title                                             first_word\n#   &lt;chr&gt;                                             &lt;chr&gt;     \n# 1 Hear Me Now                                       Hear      \n# 2 Run the World (Girls)                             Run       \n# 3 Formation                                         Formation \n# 4 7/11                                              7/11      \n# 5 My Oh My (feat. DaBaby)                           My        \n# 6 It's Automatic                                    It's      \n# 7 Poetic Justice                                    Poetic    \n# 8 A.D.H.D                                           A.D.H.D   \n# 9 Ya Estuvo                                         Ya        \n#10 Runnin (with A$AP Rocky, A$AP Ferg & Nicki Minaj) Runnin    \n\n\nWhich decades are popular for playlist_names? Using the bigspotify dataset, try doing each of these steps one at a time!\n\n\nfilter the bigspotify dataset to only include playlists that include something like “80’s” or “00’s” in their title.\ncreate a new column that extracts the decade\nuse count to find how many playlists include each decade\nwhat if you include both “80’s” and “80s”?\nhow can you count “80’s” and “80s” together in your final tibble?",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "03_strings_part2.html#grouping-and-backreferences",
    "href": "03_strings_part2.html#grouping-and-backreferences",
    "title": "Strings: In-class Exercises (Part 2)",
    "section": "Grouping and backreferences",
    "text": "Grouping and backreferences\n\n# find all fruits with repeated pair of letters.  \nfruit = stringr::fruit\nfruit\n\n [1] \"apple\"             \"apricot\"           \"avocado\"          \n [4] \"banana\"            \"bell pepper\"       \"bilberry\"         \n [7] \"blackberry\"        \"blackcurrant\"      \"blood orange\"     \n[10] \"blueberry\"         \"boysenberry\"       \"breadfruit\"       \n[13] \"canary melon\"      \"cantaloupe\"        \"cherimoya\"        \n[16] \"cherry\"            \"chili pepper\"      \"clementine\"       \n[19] \"cloudberry\"        \"coconut\"           \"cranberry\"        \n[22] \"cucumber\"          \"currant\"           \"damson\"           \n[25] \"date\"              \"dragonfruit\"       \"durian\"           \n[28] \"eggplant\"          \"elderberry\"        \"feijoa\"           \n[31] \"fig\"               \"goji berry\"        \"gooseberry\"       \n[34] \"grape\"             \"grapefruit\"        \"guava\"            \n[37] \"honeydew\"          \"huckleberry\"       \"jackfruit\"        \n[40] \"jambul\"            \"jujube\"            \"kiwi fruit\"       \n[43] \"kumquat\"           \"lemon\"             \"lime\"             \n[46] \"loquat\"            \"lychee\"            \"mandarine\"        \n[49] \"mango\"             \"mulberry\"          \"nectarine\"        \n[52] \"nut\"               \"olive\"             \"orange\"           \n[55] \"pamelo\"            \"papaya\"            \"passionfruit\"     \n[58] \"peach\"             \"pear\"              \"persimmon\"        \n[61] \"physalis\"          \"pineapple\"         \"plum\"             \n[64] \"pomegranate\"       \"pomelo\"            \"purple mangosteen\"\n[67] \"quince\"            \"raisin\"            \"rambutan\"         \n[70] \"raspberry\"         \"redcurrant\"        \"rock melon\"       \n[73] \"salal berry\"       \"satsuma\"           \"star fruit\"       \n[76] \"strawberry\"        \"tamarillo\"         \"tangerine\"        \n[79] \"ugli fruit\"        \"watermelon\"       \n\nstr_view(fruit, \"(..)\\\\1\", match = TRUE)\n\n [4] │ b&lt;anan&gt;a\n[20] │ &lt;coco&gt;nut\n[22] │ &lt;cucu&gt;mber\n[41] │ &lt;juju&gt;be\n[56] │ &lt;papa&gt;ya\n[73] │ s&lt;alal&gt; berry\n\n# why does the code below add \"pepper\" and even \"nectarine\"?\nstr_view(fruit, \"(..)(.*)\\\\1\", match = TRUE)\n\n [4] │ b&lt;anan&gt;a\n [5] │ bell &lt;peppe&gt;r\n[17] │ chili &lt;peppe&gt;r\n[20] │ &lt;coco&gt;nut\n[22] │ &lt;cucu&gt;mber\n[29] │ eld&lt;erber&gt;ry\n[41] │ &lt;juju&gt;be\n[51] │ &lt;nectarine&gt;\n[56] │ &lt;papa&gt;ya\n[73] │ s&lt;alal&gt; berry\n\n\nTips with backreference: - You must use () around the the thing you want to reference. - To backreference multiple times, use \\1 again. - The number refers to which spot you are referencing… e.g. \\2 references the second set of ()\n\nx1 &lt;- c(\"abxyba\", \"abccba\", \"xyaayx\", \"abxyab\", \"abcabc\")\nstr_view(x1, \"(.)(.)(..)\\\\2\\\\1\")\n\n[1] │ &lt;abxyba&gt;\n[2] │ &lt;abccba&gt;\n[3] │ &lt;xyaayx&gt;\n\nstr_view(x1, \"(.)(.)(..)\\\\1\\\\2\")\n\n[4] │ &lt;abxyab&gt;\n\nstr_view(x1, \"(.)(.)(.)\\\\1\\\\2\\\\3\")\n\n[5] │ &lt;abcabc&gt;\n\n\n\nDescribe to your groupmates what these expressions will match, and provide a word or expression as an example:\n\n\n(.)\\1\\1\n“(.)(.)(.).*\\3\\2\\1”\n\nWhich words in stringr::words match each expression?\n\nConstruct a regular expression to match words in stringr::words that contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice) but not match repeated pairs of numbers (e.g. 507-786-3861).\nReformat the album_release_date variable in spot_smaller so that it is MM-DD-YYYY instead of YYYY-MM-DD. (Hint: str_replace().)\nBEFORE RUNNING IT, explain to your partner(s) what the following R chunk will do:\n\n\nsentences |&gt; \n  str_replace(\"([^ ]+) ([^ ]+) ([^ ]+)\", \"\\\\1 \\\\3 \\\\2\") |&gt; \n  head(5)\n\n[1] \"The canoe birch slid on the smooth planks.\" \n[2] \"Glue sheet the to the dark blue background.\"\n[3] \"It's to easy tell the depth of a well.\"     \n[4] \"These a days chicken leg is a rare dish.\"   \n[5] \"Rice often is served in round bowls.\"",
    "crumbs": [
      "Strings: In-class Exercises (Part 2)"
    ]
  },
  {
    "objectID": "05_text_analysis.html",
    "href": "05_text_analysis.html",
    "title": "Text analysis",
    "section": "",
    "text": "You can download this .qmd file from here. Just hit the Download Raw File button.\nWe will build on techniques you learned in SDS 164 using parts of Text Mining with R by Silge and Robinson.",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#text-analysis-of-books-from-project-gutenberg",
    "href": "05_text_analysis.html#text-analysis-of-books-from-project-gutenberg",
    "title": "Text analysis",
    "section": "Text analysis of books from Project Gutenberg",
    "text": "Text analysis of books from Project Gutenberg\nWe will use the gutenbergr package to obtain several works from Project Gutenberg to examine using text analysis tools.\n\n# How I obtained the three works from Project Gutenberg\n\n# Notes:\n# - might have to find mirror at https://www.gutenberg.org/MIRRORS.ALL\n# - 84 = Frankenstein; 345 = Dracula; 43 = Jekyll and Hyde\n\n# three_works &lt;- gutenberg_download(\n#  c(84, 345, 43),\n#  meta_fields = \"title\",\n#  mirror = \"http://mirror.csclub.uwaterloo.ca/gutenberg/\")\n\n# write_csv(three_works, \"~/264_spring_2025/Data/three_works.csv\")\n\n\n# three_works &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/three_works.csv\")\n# three_works2 &lt;- read_csv(\"Data/three_works.csv\") \n\nlibrary(RCurl)\n\n\nAttaching package: 'RCurl'\n\n\nThe following object is masked from 'package:tidyr':\n\n    complete\n\nthree_works &lt;- read_csv(\n  file = getURL(\"https://raw.githubusercontent.com/proback/264_fall_2025/refs/heads/main/Data/three_works.csv\", .encoding = \"UTF-8\"))\n\nRows: 25399 Columns: 3\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, title\ndbl (1): gutenberg_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nthree_works |&gt; count(title)\n\n# A tibble: 3 × 2\n  title                                           n\n  &lt;chr&gt;                                       &lt;int&gt;\n1 Dracula                                     15491\n2 Frankenstein; Or, The Modern Prometheus      7357\n3 The Strange Case of Dr. Jekyll and Mr. Hyde  2551\n\nthree_works\n\n# A tibble: 25,399 × 3\n   gutenberg_id text                                        title               \n          &lt;dbl&gt; &lt;chr&gt;                                       &lt;chr&gt;               \n 1           43 The Strange Case Of Dr. Jekyll And Mr. Hyde The Strange Case of…\n 2           43 &lt;NA&gt;                                        The Strange Case of…\n 3           43 by Robert Louis Stevenson                   The Strange Case of…\n 4           43 &lt;NA&gt;                                        The Strange Case of…\n 5           43 &lt;NA&gt;                                        The Strange Case of…\n 6           43 Contents                                    The Strange Case of…\n 7           43 &lt;NA&gt;                                        The Strange Case of…\n 8           43 &lt;NA&gt;                                        The Strange Case of…\n 9           43 STORY OF THE DOOR                           The Strange Case of…\n10           43 &lt;NA&gt;                                        The Strange Case of…\n# ℹ 25,389 more rows\n\nfrankenstein &lt;- three_works |&gt;\n  filter(str_detect(title, \"Frankenstein\"))\n\nWe will begin by looking at a single book (Frankenstein) and then we’ll compare and contrast 3 books (Frankenstein, Dracula, and Jekyll and Hyde).",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#stop-words-get-rid-of-common-but-not-useful-words",
    "href": "05_text_analysis.html#stop-words-get-rid-of-common-but-not-useful-words",
    "title": "Text analysis",
    "section": "Stop words (get rid of common but not useful words)",
    "text": "Stop words (get rid of common but not useful words)\nNote: If you get “Error in loadNamespace(name) : there is no package called ‘stopwords’” or “The package ‘stopwords’ is required to use this function.”, first install package stopwords.\n\nget_stopwords() |&gt; print(n = 50)   # snowball is default - somewhat smaller\n\n# A tibble: 175 × 2\n   word       lexicon \n   &lt;chr&gt;      &lt;chr&gt;   \n 1 i          snowball\n 2 me         snowball\n 3 my         snowball\n 4 myself     snowball\n 5 we         snowball\n 6 our        snowball\n 7 ours       snowball\n 8 ourselves  snowball\n 9 you        snowball\n10 your       snowball\n11 yours      snowball\n12 yourself   snowball\n13 yourselves snowball\n14 he         snowball\n15 him        snowball\n16 his        snowball\n17 himself    snowball\n18 she        snowball\n19 her        snowball\n20 hers       snowball\n21 herself    snowball\n22 it         snowball\n23 its        snowball\n24 itself     snowball\n25 they       snowball\n26 them       snowball\n27 their      snowball\n28 theirs     snowball\n29 themselves snowball\n30 what       snowball\n31 which      snowball\n32 who        snowball\n33 whom       snowball\n34 this       snowball\n35 that       snowball\n36 these      snowball\n37 those      snowball\n38 am         snowball\n39 is         snowball\n40 are        snowball\n41 was        snowball\n42 were       snowball\n43 be         snowball\n44 been       snowball\n45 being      snowball\n46 have       snowball\n47 has        snowball\n48 had        snowball\n49 having     snowball\n50 do         snowball\n# ℹ 125 more rows\n\nget_stopwords(source = \"smart\") |&gt; print(n = 50)   \n\n# A tibble: 571 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           smart  \n 2 a's         smart  \n 3 able        smart  \n 4 about       smart  \n 5 above       smart  \n 6 according   smart  \n 7 accordingly smart  \n 8 across      smart  \n 9 actually    smart  \n10 after       smart  \n11 afterwards  smart  \n12 again       smart  \n13 against     smart  \n14 ain't       smart  \n15 all         smart  \n16 allow       smart  \n17 allows      smart  \n18 almost      smart  \n19 alone       smart  \n20 along       smart  \n21 already     smart  \n22 also        smart  \n23 although    smart  \n24 always      smart  \n25 am          smart  \n26 among       smart  \n27 amongst     smart  \n28 an          smart  \n29 and         smart  \n30 another     smart  \n31 any         smart  \n32 anybody     smart  \n33 anyhow      smart  \n34 anyone      smart  \n35 anything    smart  \n36 anyway      smart  \n37 anyways     smart  \n38 anywhere    smart  \n39 apart       smart  \n40 appear      smart  \n41 appreciate  smart  \n42 appropriate smart  \n43 are         smart  \n44 aren't      smart  \n45 around      smart  \n46 as          smart  \n47 aside       smart  \n48 ask         smart  \n49 asking      smart  \n50 associated  smart  \n# ℹ 521 more rows\n\n# will sometimes want to store if using over and over\n#   - later with shiny apps will have to store and write as data file\nsmart_stopwords &lt;- get_stopwords(source = \"smart\")\n\nTry out using different languages (language) and different lexicons (source).",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#another-try-at-most-common-words",
    "href": "05_text_analysis.html#another-try-at-most-common-words",
    "title": "Text analysis",
    "section": "Another try at most common words",
    "text": "Another try at most common words\n\ntidy_book |&gt;\n  anti_join(smart_stopwords) |&gt;\n  count(word, sort = TRUE) |&gt;\n  filter(word != \"NA\") |&gt;\n  slice_max(n, n = 20) |&gt;\n  ggplot(aes(fct_reorder(word, n), n)) +\n  geom_col() +\n  coord_flip()\n\nJoining with `by = join_by(word)`",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#sentiment-analysis",
    "href": "05_text_analysis.html#sentiment-analysis",
    "title": "Text analysis",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nExplore some sentiment lexicons. You’ll want to match your choice of sentiment lexicon to your purpose:\n\nafinn: scored from -5 (very negative) to +5 (very positive)\nnrc: words are labeled with emotions like anger, fear, sadness, etc. There can be more than one row per word.\nbing: binary - listed words are either negative or positive\n\n\nget_sentiments(lexicon = \"afinn\")\n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n\nget_sentiments(lexicon = \"nrc\")\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows\n\nget_sentiments(lexicon = \"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\nbing_sentiments &lt;- get_sentiments(lexicon = \"bing\")\n\nImplement sentiment analysis using an inner_join(), so you only consider words both in your text and in the lexicon.\n\ntidy_book |&gt;   \n  inner_join(bing_sentiments) |&gt;\n  count(sentiment)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 2 × 2\n  sentiment     n\n  &lt;chr&gt;     &lt;int&gt;\n1 negative   3742\n2 positive   2983\n\n\nWhat words contribute the most to sentiment scores for Frankenstein? Let’s walk through this pipe step-by-step.\n\ntidy_book |&gt;\n  inner_join(bing_sentiments) |&gt;\n  count(sentiment, word, sort = TRUE) |&gt;\n  group_by(sentiment) |&gt;\n  slice_max(n, n = 10) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment)) +\n    geom_col() +  \n    coord_flip() +\n    facet_wrap(~ sentiment, scales = \"free\")\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\n\n\n\n\n# Check out which words are associated with specific nrc emotions\nget_sentiments(\"nrc\") |&gt;\n  count(sentiment)\n\n# A tibble: 10 × 2\n   sentiment        n\n   &lt;chr&gt;        &lt;int&gt;\n 1 anger         1245\n 2 anticipation   837\n 3 disgust       1056\n 4 fear          1474\n 5 joy            687\n 6 negative      3316\n 7 positive      2308\n 8 sadness       1187\n 9 surprise       532\n10 trust         1230\n\nget_sentiments(\"nrc\") |&gt; \n  filter(sentiment == \"joy\") |&gt;\n  inner_join(tidy_book) |&gt;\n  count(word, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 308 × 2\n   word          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 found        87\n 2 friend       71\n 3 love         59\n 4 hope         50\n 5 happiness    49\n 6 happy        46\n 7 sun          45\n 8 joy          42\n 9 affection    40\n10 journey      36\n# ℹ 298 more rows\n\nget_sentiments(\"nrc\") |&gt; \n  filter(sentiment == \"anger\") |&gt;\n  inner_join(tidy_book) |&gt;\n  count(word, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 370 × 2\n   word          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 death        79\n 2 miserable    65\n 3 misery       54\n 4 words        54\n 5 despair      49\n 6 horror       45\n 7 fear         40\n 8 possessed    36\n 9 fiend        33\n10 feeling      27\n# ℹ 360 more rows\n\n\nMake a wordcloud for Frankenstein.\n\n# wordcloud wants a column with words and another column with counts\nwords &lt;- tidy_book |&gt;\n  anti_join(stop_words) |&gt;\n  count(word) |&gt;\n  filter(word != \"NA\") |&gt;\n  arrange(desc(n))\n\n# Note: this will look better in html than in the Plots window in RStudio\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 100, \n  random.order = FALSE\n)\n\n\n\n\n\n\n\n# Some alternative options\nwordcloud(\n  words = words$word, \n  freq = words$n, \n  max.words = 200, \n  random.order = FALSE, \n  rot.per = 0.35,\n  scale = c(3.5, 0.25),\n  colors = brewer.pal(6, \"Dark2\"))\n\n\n\n\n\n\n\n# Or for even cooler looks, use wordcloud2 (for html documents)\nwords_df &lt;- words |&gt;\n  slice_head(n = 80) |&gt;\n  data.frame()\n\nwordcloud2(\n  words_df, \n  size = .25, \n  shape = 'circle',\n  minSize = 10\n)\n\n\n\n\n# A couple of helpful links for customizing wordclouds:\n#   https://www.youtube.com/watch?v=0cToDzeDLRI\n#   https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a\n\nYou could do cool stuff here, like color the words by sentiment!",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#calculate-tf-idf.",
    "href": "05_text_analysis.html#calculate-tf-idf.",
    "title": "Text analysis",
    "section": "Calculate tf-idf.",
    "text": "Calculate tf-idf.\nThe tf-idf statistic is term frequency times inverse document frequency, a quantity used for identifying terms that are especially important to a particular document. The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents. We want to find words that define one document as opposed to others.\n\ntf = term frequency = proportion of times a term appears in a document.\n\nidf = inverse document frequency = log(number of documents / number of documents with the term), so that terms that appear in fewer documents are weighted higher, since those rarer words provide more information.\n\nThere’s really no theory behind multiplying the two together - it just tends to work in practice. See this wikipedia entry for more details. (See also this site for a nice description of weaknesses of tf-idf.)\n\nbook_tfidf &lt;- book_word_count |&gt;\n  bind_tf_idf(word, title, n)\n\nbook_tfidf   # note idf = 0 when it appears in every document\n\n# A tibble: 20,714 × 6\n   word  title                                       n     tf   idf tf_idf\n   &lt;chr&gt; &lt;chr&gt;                                   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 the   Dracula                                  7915 0.0480     0      0\n 2 and   Dracula                                  5907 0.0358     0      0\n 3 i     Dracula                                  4801 0.0291     0      0\n 4 to    Dracula                                  4666 0.0283     0      0\n 5 the   Frankenstein; Or, The Modern Prometheus  4195 0.0550     0      0\n 6 of    Dracula                                  3634 0.0220     0      0\n 7 and   Frankenstein; Or, The Modern Prometheus  2976 0.0391     0      0\n 8 a     Dracula                                  2954 0.0179     0      0\n 9 i     Frankenstein; Or, The Modern Prometheus  2846 0.0373     0      0\n10 of    Frankenstein; Or, The Modern Prometheus  2642 0.0347     0      0\n# ℹ 20,704 more rows\n\n\nFind high tf-idf words. The highest words will appear relatively often in one document, but not at all in others.\n\nbook_tfidf |&gt;\n  arrange(-tf_idf)\n\n# A tibble: 20,714 × 6\n   word      title                                       n      tf   idf  tf_idf\n   &lt;chr&gt;     &lt;chr&gt;                                   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 utterson  The Strange Case of Dr. Jekyll and Mr.…   128 0.00489 1.10  0.00537\n 2 jekyll    The Strange Case of Dr. Jekyll and Mr.…    84 0.00321 1.10  0.00353\n 3 poole     The Strange Case of Dr. Jekyll and Mr.…    61 0.00233 1.10  0.00256\n 4 van       Dracula                                   323 0.00196 1.10  0.00215\n 5 helsing   Dracula                                   301 0.00182 1.10  0.00200\n 6 hyde      The Strange Case of Dr. Jekyll and Mr.…    98 0.00375 0.405 0.00152\n 7 lucy      Dracula                                   223 0.00135 1.10  0.00148\n 8 mina      Dracula                                   210 0.00127 1.10  0.00140\n 9 elizabeth Frankenstein; Or, The Modern Prometheus    88 0.00115 1.10  0.00127\n10 jonathan  Dracula                                   181 0.00110 1.10  0.00120\n# ℹ 20,704 more rows\n\n\nHow can we visualize this? Let’s go step-by-step.\n\nbook_tfidf |&gt;\n  group_by(title) |&gt;\n  arrange(desc(tf_idf)) |&gt;\n  slice_max(tf_idf, n = 10) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(word, tf_idf), y = tf_idf, fill = title)) +\n    geom_col(show.legend = FALSE) +\n    coord_flip() +\n    facet_wrap(~title, scales = \"free\")\n\n\n\n\n\n\n\n# kind of boring - mostly proper nouns",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#n-grams-and-beyond",
    "href": "05_text_analysis.html#n-grams-and-beyond",
    "title": "Text analysis",
    "section": "N-grams… and beyond!",
    "text": "N-grams… and beyond!\nLet’s return to Frankenstein and look at 2-word combinations:\n\ntidy_ngram &lt;- frankenstein |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) |&gt;\n  filter(bigram != \"NA\")\n\ntidy_ngram\n\n# A tibble: 68,847 × 3\n   gutenberg_id title                                   bigram               \n          &lt;dbl&gt; &lt;chr&gt;                                   &lt;chr&gt;                \n 1           84 Frankenstein; Or, The Modern Prometheus or the               \n 2           84 Frankenstein; Or, The Modern Prometheus the modern           \n 3           84 Frankenstein; Or, The Modern Prometheus modern prometheus    \n 4           84 Frankenstein; Or, The Modern Prometheus by mary              \n 5           84 Frankenstein; Or, The Modern Prometheus mary wollstonecraft  \n 6           84 Frankenstein; Or, The Modern Prometheus wollstonecraft godwin\n 7           84 Frankenstein; Or, The Modern Prometheus godwin shelley       \n 8           84 Frankenstein; Or, The Modern Prometheus letter 1             \n 9           84 Frankenstein; Or, The Modern Prometheus letter 2             \n10           84 Frankenstein; Or, The Modern Prometheus letter 3             \n# ℹ 68,837 more rows\n\n\nWhat are the most common bigrams?\n\ntidy_ngram |&gt;\n  count(bigram, sort = TRUE)\n\n# A tibble: 38,574 × 2\n   bigram      n\n   &lt;chr&gt;   &lt;int&gt;\n 1 of the    501\n 2 of my     264\n 3 in the    246\n 4 i was     213\n 5 i had     207\n 6 that i    198\n 7 and i     192\n 8 and the   182\n 9 to the    181\n10 which i   145\n# ℹ 38,564 more rows\n\n\nLet’s use separate() from tidyr to remove stop words.\n\n# stop_words contains 1149 words from 3 lexicons\nbigrams_filtered &lt;- tidy_ngram |&gt;\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |&gt;\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word) |&gt;\n  count(word1, word2, sort = TRUE)\nbigrams_filtered\n\n# A tibble: 4,677 × 3\n   word1       word2          n\n   &lt;chr&gt;       &lt;chr&gt;      &lt;int&gt;\n 1 natural     philosophy    11\n 2 dear        victor        10\n 3 native      country       10\n 4 de          lacey          9\n 5 fellow      creatures      8\n 6 poor        girl           8\n 7 mont        blanc          7\n 8 native      town           6\n 9 cornelius   agrippa        5\n10 countenance expressed      5\n# ℹ 4,667 more rows\n\n\nNow extend from a single document to our collection of documents. See which two-word combinations best identify books in the collection.\n\nbook_twowords &lt;- three_works |&gt;\n  group_by(title) |&gt;\n  mutate(linenumber = row_number()) |&gt;\n  ungroup() |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) |&gt;\n  filter(bigram != \"NA\")\n \nbook_twowords |&gt;\n  count(bigram, sort = TRUE)\n\n# A tibble: 102,837 × 2\n   bigram      n\n   &lt;chr&gt;   &lt;int&gt;\n 1 of the   1494\n 2 in the    952\n 3 to the    596\n 4 and the   579\n 5 and i     554\n 6 it was    526\n 7 that i    526\n 8 on the    507\n 9 i was     484\n10 i had     461\n# ℹ 102,827 more rows\n\nbigrams_filtered &lt;- book_twowords |&gt;\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |&gt;\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word) |&gt;\n  count(word1, word2, sort = TRUE) |&gt;\n  filter(!is.na(word1) & !is.na(word2))\n\nbigrams_filtered \n\n# A tibble: 13,951 × 3\n   word1    word2         n\n   &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;\n 1 van      helsing     282\n 2 madam    mina         82\n 3 lord     godalming    63\n 4 dr       van          60\n 5 dr       seward       55\n 6 friend   john         54\n 7 seward's diary        39\n 8 poor     dear         34\n 9 harker's journal      31\n10 _dr      seward's     26\n# ℹ 13,941 more rows\n\nbigrams_united &lt;- bigrams_filtered |&gt;\n  unite(bigram, word1, word2, sep = \" \")\n\nbigrams_united \n\n# A tibble: 13,951 × 2\n   bigram               n\n   &lt;chr&gt;            &lt;int&gt;\n 1 van helsing        282\n 2 madam mina          82\n 3 lord godalming      63\n 4 dr van              60\n 5 dr seward           55\n 6 friend john         54\n 7 seward's diary      39\n 8 poor dear           34\n 9 harker's journal    31\n10 _dr seward's        26\n# ℹ 13,941 more rows\n\nbigram_tf_idf &lt;- book_twowords |&gt;\n  count(title, bigram) |&gt;\n  bind_tf_idf(bigram, title, n) |&gt;\n  arrange(desc(tf_idf)) \n\nbigram_tf_idf |&gt; arrange(desc(tf_idf))\n\n# A tibble: 119,039 × 6\n   title                                      bigram     n      tf   idf  tf_idf\n   &lt;chr&gt;                                      &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 The Strange Case of Dr. Jekyll and Mr. Hy… mr ut…    69 2.92e-3  1.10 3.21e-3\n 2 The Strange Case of Dr. Jekyll and Mr. Hy… the l…    61 2.58e-3  1.10 2.84e-3\n 3 Dracula                                    van h…   282 1.88e-3  1.10 2.07e-3\n 4 The Strange Case of Dr. Jekyll and Mr. Hy… mr hy…    29 1.23e-3  1.10 1.35e-3\n 5 The Strange Case of Dr. Jekyll and Mr. Hy… dr je…    23 9.74e-4  1.10 1.07e-3\n 6 The Strange Case of Dr. Jekyll and Mr. Hy… henry…    22 9.32e-4  1.10 1.02e-3\n 7 The Strange Case of Dr. Jekyll and Mr. Hy… edwar…    20 8.47e-4  1.10 9.30e-4\n 8 Dracula                                    the c…   121 8.08e-4  1.10 8.88e-4\n 9 The Strange Case of Dr. Jekyll and Mr. Hy… the c…    16 6.78e-4  1.10 7.44e-4\n10 The Strange Case of Dr. Jekyll and Mr. Hy… of ed…    14 5.93e-4  1.10 6.51e-4\n# ℹ 119,029 more rows\n\nbigram_tf_idf |&gt;\n  group_by(title) |&gt;\n  arrange(desc(tf_idf)) |&gt;\n  slice_max(tf_idf, n = 10) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x = fct_reorder(bigram, tf_idf), y = tf_idf, fill = title)) +\n    geom_col(show.legend = FALSE) +\n    coord_flip() +\n    facet_wrap(~title, scales = \"free\")",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#sentence-context-using-bigrams",
    "href": "05_text_analysis.html#sentence-context-using-bigrams",
    "title": "Text analysis",
    "section": "Sentence context using bigrams",
    "text": "Sentence context using bigrams\nBigrams can also help us dive deeper into sentiment analysis. For example, even though “happy” carries positive sentiment, but when preceded by “not” as in this sentence: “I am not happy with you!” it conveys negative sentiment. Context can matter as much as mere presence!\nLet’s see which words associated with an afinn sentiment are most frequently preceded by “not”:\n\nafinn &lt;- get_sentiments(\"afinn\")\n\nbigrams_separated &lt;- book_twowords |&gt;\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |&gt;\n  count(word1, word2, sort = TRUE) |&gt;\n  filter(!is.na(word1) & !is.na(word2))\n\nbigrams_separated |&gt; filter(word1 == \"not\")\n\n# A tibble: 582 × 3\n   word1 word2     n\n   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1 not   be       77\n 2 not   to       74\n 3 not   know     62\n 4 not   so       38\n 5 not   have     36\n 6 not   a        35\n 7 not   yet      34\n 8 not   the      31\n 9 not   for      29\n10 not   been     26\n# ℹ 572 more rows\n\nnot_words &lt;- bigrams_separated |&gt;\n  filter(word1 == \"not\") |&gt;\n  inner_join(afinn, by = c(word2 = \"word\")) |&gt;\n  arrange(desc(n))\n\nnot_words\n\n# A tibble: 123 × 4\n   word1 word2       n value\n   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n 1 not   like       19     2\n 2 not   want       14     1\n 3 not   fear       13    -2\n 4 not   help       11     2\n 5 not   wish        9     1\n 6 not   afraid      7    -2\n 7 not   care        7     2\n 8 not   fail        7    -2\n 9 not   leave       7    -1\n10 not   despair     6    -3\n# ℹ 113 more rows\n\n\nWe could then ask which words contributed the most in the “wrong” direction. One approach is to multiply their value by the number of times they appear (so that a word with a value of +3 occurring 10 times has as much impact as a word with a sentiment value of +1 occurring 30 times).\n\nnot_words |&gt;\n  mutate(contribution = n * value) |&gt;\n  arrange(desc(abs(contribution))) |&gt;\n  head(20) |&gt;\n  mutate(word2 = reorder(word2, contribution)) |&gt;\n  ggplot(aes(n * value, word2, fill = n * value &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Sentiment value * number of occurrences\",\n       y = \"Words preceded by \\\"not\\\"\")\n\n\n\n\n\n\n\n\nWith this approach, we could expand our list of negation words, and then possibly even adjust afinn totals to reflect context!\n\n# An example of expanding the list of negation words\nnegation_words &lt;- c(\"not\", \"no\", \"never\", \"without\")\n\nnegated_words &lt;- bigrams_separated |&gt;\n  filter(word1 %in% negation_words) |&gt;\n  inner_join(afinn, by = c(word2 = \"word\")) |&gt;\n  arrange(desc(n))\n\nnegated_words\n\n# A tibble: 232 × 4\n   word1 word2      n value\n   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n 1 not   like      19     2\n 2 not   want      14     1\n 3 not   fear      13    -2\n 4 no    matter    12     1\n 5 no    doubt     11    -1\n 6 no    no        11    -1\n 7 not   help      11     2\n 8 no    fear       9    -2\n 9 not   wish       9     1\n10 not   afraid     7    -2\n# ℹ 222 more rows\n\nnegated_words |&gt;\n  mutate(contribution = n * value) |&gt;\n  arrange(desc(abs(contribution))) |&gt;\n  group_by(word1) |&gt;\n  slice_max(abs(contribution), n = 10) |&gt;\n  ungroup() |&gt;\n  mutate(word2 = reorder(word2, contribution)) |&gt;\n  ggplot(aes(n * value, word2, fill = n * value &gt; 0)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ word1, scales = \"free\") +\n    labs(x = \"Sentiment value * number of occurrences\",\n         y = \"Words preceded by negation term\")",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#creating-a-network-graph",
    "href": "05_text_analysis.html#creating-a-network-graph",
    "title": "Text analysis",
    "section": "Creating a network graph",
    "text": "Creating a network graph\nIf we are interested in visualizing all relationships among words or bigrams, we can arrange the words into a network, which is a combination of connected nodes. A network graph has three elements:\n\nfrom: the node an edge is coming from\nto: the node an edge is going towards\nweight: A numeric value associated with each edge\n\nThe igraph package has many powerful functions for manipulating and analyzing networks. One way to create an igraph object from tidy data is the graph_from_data_frame() function. Let’s see how it works using Frankenstein:\n\nlibrary(igraph)\n\n# filter for only relatively common combinations\nbigram_graph &lt;- bigrams_filtered |&gt;\n  filter(n &gt; 10) |&gt;\n  graph_from_data_frame()\n\nbigram_graph\n\nIGRAPH d713a9f DN-- 37 27 -- \n+ attr: name (v/c), n (e/n)\n+ edges from d713a9f (vertex names):\n [1] van     -&gt;helsing    madam   -&gt;mina       lord    -&gt;godalming \n [4] dr      -&gt;van        dr      -&gt;seward     friend  -&gt;john      \n [7] seward's-&gt;diary      poor    -&gt;dear       harker's-&gt;journal   \n[10] _dr     -&gt;seward's   dear    -&gt;madam      miss    -&gt;lucy      \n[13] dr      -&gt;jekyll     henry   -&gt;jekyll     poor    -&gt;lucy      \n[16] quincey -&gt;morris     edward  -&gt;hyde       dr      -&gt;seward's  \n[19] van     -&gt;helsing's  _czarina-&gt;catherine_ poor    -&gt;fellow    \n[22] _mina   -&gt;harker's   poor    -&gt;girl       dr      -&gt;seward’s  \n+ ... omitted several edges\n\n# Use ggraph to convert into a network plot\nlibrary(ggraph)\nset.seed(2017)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1)\n\n\n\n\n\n\n\n# polish the graph\nset.seed(2020)\na &lt;- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,\n                 arrow = a, end_cap = circle(.07, 'inches')) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n  theme_void()",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#correlating-pairs-of-words",
    "href": "05_text_analysis.html#correlating-pairs-of-words",
    "title": "Text analysis",
    "section": "Correlating pairs of words",
    "text": "Correlating pairs of words\nTokenizing by n-gram is a useful way to explore pairs of adjacent words. However, we may also be interested in words that tend to co-occur within particular documents or particular chapters, even if they don’t occur next to each other. Following Section 4.2 of Text Mining with R, we will use the widyr package.\nConsider the book “Frankenstein” divided into 10-line sections. We may be interested in what words tend to appear within the same section.\n\nfrankenstein_section_words &lt;- frankenstein |&gt;\n  select(-gutenberg_id) |&gt;\n  mutate(section = row_number() %/% 10) |&gt; \n  filter(section &gt; 0) |&gt;\n  unnest_tokens(word, text) |&gt; \n  filter(!word %in% stop_words$word,\n         !is.na(word))\n\nfrankenstein_section_words \n\n# A tibble: 27,313 × 3\n   title                                   section word   \n   &lt;chr&gt;                                     &lt;dbl&gt; &lt;chr&gt;  \n 1 Frankenstein; Or, The Modern Prometheus       1 letter \n 2 Frankenstein; Or, The Modern Prometheus       1 1      \n 3 Frankenstein; Or, The Modern Prometheus       1 letter \n 4 Frankenstein; Or, The Modern Prometheus       1 2      \n 5 Frankenstein; Or, The Modern Prometheus       1 letter \n 6 Frankenstein; Or, The Modern Prometheus       1 3      \n 7 Frankenstein; Or, The Modern Prometheus       1 letter \n 8 Frankenstein; Or, The Modern Prometheus       1 4      \n 9 Frankenstein; Or, The Modern Prometheus       1 chapter\n10 Frankenstein; Or, The Modern Prometheus       1 1      \n# ℹ 27,303 more rows\n\n# count words co-occuring within sections\nlibrary(widyr)\n\nWarning: package 'widyr' was built under R version 4.5.1\n\nword_pairs &lt;- frankenstein_section_words |&gt;\n  pairwise_count(word, section, sort = TRUE)\n\nword_pairs\n\n# A tibble: 856,676 × 3\n   item1     item2         n\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 elizabeth father       20\n 2 father    elizabeth    20\n 3 life      death        19\n 4 death     life         19\n 5 eyes      life         18\n 6 justine   poor         18\n 7 life      eyes         18\n 8 poor      justine      18\n 9 elizabeth dear         17\n10 native    country      17\n# ℹ 856,666 more rows\n\n# What words occur most often with \"life\"?\nword_pairs |&gt;\n  filter(item1 == \"life\")\n\n# A tibble: 2,330 × 3\n   item1 item2        n\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1 life  death       19\n 2 life  eyes        18\n 3 life  friend      16\n 4 life  father      16\n 5 life  mind        14\n 6 life  day         13\n 7 life  feelings    13\n 8 life  found       13\n 9 life  time        12\n10 life  passed      12\n# ℹ 2,320 more rows\n\n\nWe can quantify pairwise correlation using the Phi coefficient (which simplifies to the Pearson correlation coefficient with numeric data). The Phi coefficient measures how often two words appear together relative to how often they appear separately (so we don’t just pick up the most common words).\n\n# we need to filter for at least relatively common words first\nword_cors &lt;- frankenstein_section_words |&gt;\n  group_by(word) |&gt;\n  filter(n() &gt;= 10) |&gt;\n  pairwise_cor(word, section, sort = TRUE)\n\nword_cors\n\n# A tibble: 406,406 × 3\n   item1      item2      correlation\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt;\n 1 philosophy natural          0.703\n 2 natural    philosophy       0.703\n 3 thou       thy              0.550\n 4 thy        thou             0.550\n 5 understood language         0.499\n 6 language   understood       0.499\n 7 felix      agatha           0.470\n 8 agatha     felix            0.470\n 9 creatures  fellow           0.465\n10 fellow     creatures        0.465\n# ℹ 406,396 more rows\n\n# What words are most correlated with \"life\"?\nword_cors |&gt;\n  filter(item1 == \"life\")\n\n# A tibble: 637 × 3\n   item1 item2    correlation\n   &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1 life  story          0.120\n 2 life  bestowed       0.120\n 3 life  frame          0.112\n 4 life  death          0.111\n 5 life  purpose        0.109\n 6 life  dream          0.109\n 7 life  creation       0.109\n 8 life  deprived       0.108\n 9 life  hideous        0.108\n10 life  money          0.105\n# ℹ 627 more rows\n\n\nPlot words most associated with a set of interesting words:\n\nword_cors |&gt;\n  filter(item1 %in% c(\"life\", \"death\", \"father\", \"eyes\")) |&gt;\n  group_by(item1) |&gt;\n  slice_max(correlation, n = 6) |&gt;\n  ungroup() |&gt;\n  mutate(item2 = reorder(item2, correlation)) |&gt;\n  ggplot(aes(item2, correlation)) +\n    geom_bar(stat = \"identity\") +\n    facet_wrap(~ item1, scales = \"free\") +\n    coord_flip()\n\n\n\n\n\n\n\n\nFinally, create a network graph to visualize the correlations and clusters of words that were found by the widyr package\n\nset.seed(2016)\n\nword_cors |&gt;\n  filter(correlation &gt; .25) |&gt;\n  graph_from_data_frame() |&gt;\n  ggraph(layout = \"fr\") +\n    geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n    geom_node_point(color = \"lightblue\", size = 5) +\n    geom_node_text(aes(label = name), repel = TRUE) +\n    theme_void()",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#topic-modeling",
    "href": "05_text_analysis.html#topic-modeling",
    "title": "Text analysis",
    "section": "Topic Modeling",
    "text": "Topic Modeling\nAs described in Ch 6 of Text Mining with R:\n\nIn text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.\n\n\nLatent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.\n\nWe will attempt to apply LDA to our collection of three works. While not a typical application of topic modeling, it’ll be interesting to see if any common themes or groupings emerge.\nAgain, from Ch 6:\n\nLatent Dirichlet allocation is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.\n\n\n\nEvery document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”\n\n\n\n\nEvery topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.\n\n\n\nLDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.\n\nIn order to implement LDA on our three books, we need to first “cast” our tidy data as a document-term matrix (DTM) where:\n\neach row represents one document (such as a book or article),\neach column represents one term, and\neach value (typically) contains the number of appearances of that term in that document.\n\nFrom Section 5.2 of Text Mining with R:\n\nSince most pairings of document and term do not occur (they have the value zero), DTMs are usually implemented as sparse matrices. These objects can be treated as though they were matrices (for example, accessing particular rows and columns), but are stored in a more efficient format.\n\n\nDTM objects cannot be used directly with tidy tools, just as tidy data frames cannot be used as input for most text mining packages. Thus, the tidytext package provides two verbs (tidy and cast) that convert between the two formats.\n\n\nA DTM is typically comparable to a tidy data frame after a count or a group_by/summarize that contains counts or another statistic for each combination of a term and document.\n\n\n# cast the collection of 3 works as a document-term matrix\nlibrary(tm)\n\nWarning: package 'tm' was built under R version 4.5.1\n\n\nLoading required package: NLP\n\n\n\nAttaching package: 'NLP'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\nthree_books_dtm &lt;- book_word_count |&gt;\n  filter(!word %in% stop_words$word,\n         !is.na(word)) |&gt;\n  cast_dtm(title, word, n)\n\n# set a seed so that the output of the model is predictable\nlibrary(topicmodels)\n\nWarning: package 'topicmodels' was built under R version 4.5.1\n\nthree_books_lda &lt;- LDA(three_books_dtm, k = 2, control = list(seed = 1234))\nthree_books_lda\n\nA LDA_VEM topic model with 2 topics.\n\n\nAfter fitting our LDA model, we will first focus on the beta variable, which is the probability of a word being generated by a specific topic. Then we’ll turn to the gamma variable, which are the per-document per-topic probabilities, or the proportion of words from a document generated by a specific topic.\n\nthree_books_topics &lt;- tidy(three_books_lda, matrix = \"beta\")\nthree_books_topics\n\n# A tibble: 25,968 × 3\n   topic term        beta\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1     1 time    3.58e- 3\n 2     2 time    8.03e- 3\n 3     1 van     1.26e-14\n 4     2 van     6.65e- 3\n 5     1 night   3.41e- 3\n 6     2 night   6.39e- 3\n 7     1 helsing 1.91e-14\n 8     2 helsing 6.20e- 3\n 9     1 dear    2.18e- 3\n10     2 dear    4.61e- 3\n# ℹ 25,958 more rows\n\n# Find the most common words within each topic\nthree_books_top_terms &lt;- three_books_topics |&gt;\n  group_by(topic) |&gt;\n  slice_max(beta, n = 10) |&gt; \n  ungroup() |&gt;\n  arrange(topic, -beta)\n\nthree_books_top_terms |&gt;\n  mutate(term = reorder_within(term, beta, topic)) |&gt;\n  ggplot(aes(beta, term, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    scale_y_reordered()\n\n\n\n\n\n\n\n# This would be much cooler with more documents and if we were able\n#   to anti_join to remove proper nouns\n\n# Find words with greatest difference between two topics, using log ratio\nbeta_wide &lt;- three_books_topics |&gt;\n  mutate(topic = paste0(\"topic\", topic)) |&gt;\n  pivot_wider(names_from = topic, values_from = beta) |&gt; \n  filter(topic1 &gt; .001 | topic2 &gt; .001) |&gt;\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide\n\n# A tibble: 196 × 4\n   term      topic1  topic2 log_ratio\n   &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 time    3.58e- 3 0.00803     1.17 \n 2 van     1.26e-14 0.00665    38.9  \n 3 night   3.41e- 3 0.00639     0.906\n 4 helsing 1.91e-14 0.00620    38.2  \n 5 dear    2.18e- 3 0.00461     1.08 \n 6 lucy    5.62e-14 0.00459    36.3  \n 7 day     2.99e- 3 0.00455     0.607\n 8 hand    1.98e- 3 0.00433     1.12 \n 9 mina    2.40e-14 0.00433    37.4  \n10 door    2.12e- 3 0.00412     0.956\n# ℹ 186 more rows\n\nbeta_wide |&gt;\n  arrange(desc(abs(log_ratio))) |&gt;\n  slice_max(abs(log_ratio), n = 20) |&gt;\n  mutate(term = reorder(term, log_ratio)) |&gt;\n  ggplot(aes(log_ratio, term, fill = log_ratio &gt; 0)) +\n    geom_col(show.legend = FALSE) +\n    labs(x = \"Log ratio of Beta values\",\n         y = \"Words in three works\")\n\n\n\n\n\n\n\n# find the gamma variable for each document and topic\nthree_books_documents &lt;- tidy(three_books_lda, matrix = \"gamma\")\nthree_books_documents\n\n# A tibble: 6 × 3\n  document                                    topic      gamma\n  &lt;chr&gt;                                       &lt;int&gt;      &lt;dbl&gt;\n1 Dracula                                         1 0.000158  \n2 The Strange Case of Dr. Jekyll and Mr. Hyde     1 1.00      \n3 Frankenstein; Or, The Modern Prometheus         1 1.00      \n4 Dracula                                         2 1.00      \n5 The Strange Case of Dr. Jekyll and Mr. Hyde     2 0.00000613\n6 Frankenstein; Or, The Modern Prometheus         2 0.00000190\n\n# Dracula = Topic 2; other two books = Topic 1!",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#a-few-analyses-from-sds-164",
    "href": "05_text_analysis.html#a-few-analyses-from-sds-164",
    "title": "Text analysis",
    "section": "A few analyses from SDS 164:",
    "text": "A few analyses from SDS 164:\n\n# 10 most common words in each book, excluding stop words\npotter_tidy |&gt;\n  count(title, word) |&gt;\n  anti_join(stop_words) |&gt;\n  group_by(title) |&gt;\n  slice_max(n, n = 10) |&gt;\n  mutate(rank = 1:10) |&gt;\n   select(-n) |&gt;\n  pivot_wider (names_from = title, values_from = word) |&gt;\n  print(width = Inf)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 10 × 8\n    rank `Sorcerer's Stone` `Chamber of Secrets` `Prisoner of Azkaban`\n   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;                &lt;chr&gt;                \n 1     1 harry              harry                harry                \n 2     2 ron                ron                  ron                  \n 3     3 hagrid             hermione             hermione             \n 4     4 hermione           malfoy               professor            \n 5     5 professor          lockhart             lupin                \n 6     6 looked             professor            black                \n 7     7 snape              weasley              looked               \n 8     8 dumbledore         looked               hagrid               \n 9     9 uncle              time                 snape                \n10    10 time               eyes                 harry's              \n   `Goblet of Fire` `Order of the Phoenix` `Half-Blood Prince` `Deathly Hallows`\n   &lt;chr&gt;            &lt;chr&gt;                  &lt;chr&gt;               &lt;chr&gt;            \n 1 harry            harry                  harry               harry            \n 2 ron              hermione               dumbledore          hermione         \n 3 hermione         ron                    ron                 ron              \n 4 dumbledore       sirius                 hermione            wand             \n 5 looked           professor              looked              dumbledore       \n 6 weasley          dumbledore             slughorn            looked           \n 7 hagrid           looked                 snape               voldemort        \n 8 eyes             umbridge               malfoy              eyes             \n 9 moody            weasley                time                death            \n10 professor        voice                  professor           time             \n\n# Repeat above after removing character first and last names\npotter_tidy |&gt;\n  count(title, word) |&gt;\n  anti_join(stop_words) |&gt;\n  anti_join(potter_names, join_by(word == firstname)) |&gt; \n  anti_join(potter_names, join_by(word == lastname)) |&gt;\n  group_by(title) |&gt;\n  slice_max(n, n = 10, with_ties = FALSE) |&gt;\n  mutate(rank = 1:10) |&gt;\n   select(-n) |&gt;\n  pivot_wider (names_from = title, values_from = word) |&gt;\n  print(width = Inf)\n\nJoining with `by = join_by(word)`\n\n\n# A tibble: 10 × 8\n    rank `Sorcerer's Stone` `Chamber of Secrets` `Prisoner of Azkaban`\n   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;                &lt;chr&gt;                \n 1     1 professor          professor            professor            \n 2     2 looked             looked               looked               \n 3     3 uncle              time                 harry's              \n 4     4 time               eyes                 eyes                 \n 5     5 harry's            harry's              time                 \n 6     6 door               dobby                door                 \n 7     7 eyes               door                 head                 \n 8     8 yeh                head                 voice                \n 9     9 head               voice                heard                \n10    10 told               school               hand                 \n   `Goblet of Fire` `Order of the Phoenix` `Half-Blood Prince` `Deathly Hallows`\n   &lt;chr&gt;            &lt;chr&gt;                  &lt;chr&gt;               &lt;chr&gt;            \n 1 looked           professor              looked              wand             \n 2 eyes             looked                 time                looked           \n 3 professor        voice                  professor           eyes             \n 4 crouch           time                   hand                death            \n 5 time             door                   eyes                time             \n 6 wand             head                   voice               voice            \n 7 voice            harry's                dark                harry's          \n 8 head             eyes                   wand                door             \n 9 told             wand                   door                hand             \n10 harry's          hand                   head                head             \n\n# still get \"harry's\" and \"professor\" but otherwise looks good\n\n# top 10 names in each book (after excluding \"the\")\npotter_tidy |&gt;\n  count(title, word) |&gt;\n  semi_join(potter_names, join_by(word == firstname)) |&gt;\n  filter(word != \"the\") |&gt; # ADD for #6\n  group_by(title) |&gt;\n  slice_max(n, n = 10, with_ties = FALSE) |&gt;\n  mutate(rank = 1:10) |&gt;\n   select(-n) |&gt;\n  pivot_wider (names_from = title, values_from = word) |&gt;\n  print(width = Inf)\n\n# A tibble: 10 × 8\n    rank `Sorcerer's Stone` `Chamber of Secrets` `Prisoner of Azkaban`\n   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;                &lt;chr&gt;                \n 1     1 harry              harry                harry                \n 2     2 ron                ron                  ron                  \n 3     3 hermione           hermione             hermione             \n 4     4 dudley             fred                 sirius               \n 5     5 vernon             ginny                neville              \n 6     6 neville            sir                  madam                \n 7     7 great              george               great                \n 8     8 petunia            great                fred                 \n 9     9 nearly             percy                vernon               \n10    10 madam              nearly               percy                \n   `Goblet of Fire` `Order of the Phoenix` `Half-Blood Prince` `Deathly Hallows`\n   &lt;chr&gt;            &lt;chr&gt;                  &lt;chr&gt;               &lt;chr&gt;            \n 1 harry            harry                  harry               harry            \n 2 ron              hermione               ron                 hermione         \n 3 hermione         ron                    hermione            ron              \n 4 cedric           sirius                 ginny               great            \n 5 sirius           fred                   great               lord             \n 6 fred             george                 sir                 luna             \n 7 great            neville                lord                bill             \n 8 george           ginny                  fred                ginny            \n 9 percy            great                  tom                 albus            \n10 rita             luna                   draco               fred             \n\n# spell statistics by book\npotter_tidy |&gt;\n  left_join(potter_spells, join_by(word == first_word)) |&gt;\n  group_by(title) |&gt;\n  summarize(num_spells_cast = sum(!is.na(spell_name)), \n            spells_per_10kwords = mean(!is.na(spell_name)) * 10000,\n            num_unique_spells = n_distinct(spell_name) - 1)  # Why -1??\n\n# A tibble: 7 × 4\n  title                num_spells_cast spells_per_10kwords num_unique_spells\n  &lt;fct&gt;                          &lt;int&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1 Sorcerer's Stone                   7               0.899                 4\n2 Chamber of Secrets                12               1.41                  9\n3 Prisoner of Azkaban               65               6.17                 14\n4 Goblet of Fire                    67               3.49                 27\n5 Order of the Phoenix              94               3.63                 28\n6 Half-Blood Prince                 65               3.79                 24\n7 Deathly Hallows                  114               5.77                 34\n\n# plot of top spells by book\npotter_tidy |&gt;\n  left_join(potter_spells, join_by(word == first_word)) |&gt;\n  drop_na(spell_name) |&gt;  \n  mutate(spell_name = fct_infreq(spell_name),\n         spell_name = fct_lump_n(spell_name, n = 5)) |&gt;\n    count(title, spell_name) |&gt;\n  ggplot() +\n  geom_col(aes(x = title, y = n, fill = spell_name), position = \"stack\")",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "05_text_analysis.html#new-stuff",
    "href": "05_text_analysis.html#new-stuff",
    "title": "Text analysis",
    "section": "New stuff!",
    "text": "New stuff!\n\nWhat words contribute the most to negative and positive sentiment scores? Show a faceted bar plot of the top 10 negative and the top 10 positive words (according to the “bing” lexicon) across the entire series.\nFind a list of the top 10 words associated with “fear” and with “trust” (according to the “nrc” lexicon) across the entire series.\nMake a wordcloud for the entire series after removing stop words using the “smart” source.\nCreate a wordcloud with the top 20 negative words and the top 20 positive words in the Harry Potter series according to the bing lexicon. The words should be sized by their respective counts and colored based on whether their sentiment is positive or negative. (Feel free to be resourceful and creative to color words by a third variable!)\nMake a faceted bar chart to compare the positive/negative sentiment trajectory over the 7 Harry Potter books. You should have one bar per chapter (thus chapter becomes the index), and the bar should extend up from 0 if there are more positive than negative words in a chapter (according to the “bing” lexicon), and it will extend down from 0 if there are more negative than positive words.\nRepeat (5) using a faceted scatterplot to show the average sentiment score according to the “afinn” lexicon for each chapter. (Hint: use mutate(chapter_factor = factor(chapter)) to treat chapter as a factor variable.)\nMake a faceted bar plot showing the top 10 words that distinguish each book according to the tf-idf statistic.\nRepeat (7) to show the top 10 2-word combinations that distinguish each book.\nFind which words contributed most in the “wrong” direction using the afinn sentiment combined with how often a word appears among all 7 books. Come up with a list of 4 negation words, and for each negation word, illustrate the words associated with the largest “wrong” contributions in a faceted bar plot.\nSelect a set of 4 “interesting” terms and then use the Phi coefficient to find and plot the 6 words most correlated with each of your “interesting” words. Start by dividing potter_tidy into 80-word sections and then remove names and spells and stop words.\nCreate a network graph to visualize the correlations and clusters of words that were found by the widyr package in (10).\nUse LDA to fit a 2-topic model to all 7 Harry Potter books. Be sure to remove names, spells, and stop words before running your topic models. (a) Make a plot to illustrate words with greatest difference between two topics, using log ratio. (b) Print a table with the gamma variable for each document and topic. Based on (a) and (b), can you interpret what the two topics represent?",
    "crumbs": [
      "Text analysis"
    ]
  },
  {
    "objectID": "github_links.html",
    "href": "github_links.html",
    "title": "GitHub Links",
    "section": "",
    "text": "Here are a few additional GitHub links that I’ve found helpful:\n\nUsing git and GitHub with RStudio (Lisa Lendway)\nGitHub with R projects (Lisa Lendway)\nGitHub starter course – basic terminology\nHappy Git and GitHub for the useR (Jenny Bryan)",
    "crumbs": [
      "GitHub Links"
    ]
  },
  {
    "objectID": "miniproject01.html",
    "href": "miniproject01.html",
    "title": "Mini-Project 1: Text Analysis",
    "section": "",
    "text": "You will find a data set containing string data. This could be newspaper articles, tweets, songs, plays, movie reviews, or anything else you can imagine. Then you will answer questions of interest and tell a story about your data using skills you have developed in strings, regular expressions, and text analysis.\nYour story must contain the following elements:\n\nat least 3 different str_ functions\nat least 3 different regular expressions\nat least 2 different text analysis applications (count words, bing sentiment, afinn sentiment, nrc sentiment, wordclouds, trajectories over sections or time, tf-idf, bigrams, correlations, networks, LDA, etc.). Note that many interesting insights can be gained by strategic and thoughtful use of regular expressions paired with simple counts and summary statistics.\nat least 3 illustrative, well-labeled plots or tables\na description of what insights can be gained from your plots and tables. Be sure you weave a compelling and interesting story!\n\nBe sure to highlight the elements above so that they are easy for me to spot!",
    "crumbs": [
      "Mini-Project 1: Text Analysis"
    ]
  },
  {
    "objectID": "miniproject01.html#overview",
    "href": "miniproject01.html#overview",
    "title": "Mini-Project 1: Text Analysis",
    "section": "",
    "text": "You will find a data set containing string data. This could be newspaper articles, tweets, songs, plays, movie reviews, or anything else you can imagine. Then you will answer questions of interest and tell a story about your data using skills you have developed in strings, regular expressions, and text analysis.\nYour story must contain the following elements:\n\nat least 3 different str_ functions\nat least 3 different regular expressions\nat least 2 different text analysis applications (count words, bing sentiment, afinn sentiment, nrc sentiment, wordclouds, trajectories over sections or time, tf-idf, bigrams, correlations, networks, LDA, etc.). Note that many interesting insights can be gained by strategic and thoughtful use of regular expressions paired with simple counts and summary statistics.\nat least 3 illustrative, well-labeled plots or tables\na description of what insights can be gained from your plots and tables. Be sure you weave a compelling and interesting story!\n\nBe sure to highlight the elements above so that they are easy for me to spot!",
    "crumbs": [
      "Mini-Project 1: Text Analysis"
    ]
  },
  {
    "objectID": "miniproject01.html#evaluation-rubric",
    "href": "miniproject01.html#evaluation-rubric",
    "title": "Mini-Project 1: Text Analysis",
    "section": "Evaluation Rubric",
    "text": "Evaluation Rubric\nAvailable here",
    "crumbs": [
      "Mini-Project 1: Text Analysis"
    ]
  },
  {
    "objectID": "miniproject01.html#timeline",
    "href": "miniproject01.html#timeline",
    "title": "Mini-Project 1: Text Analysis",
    "section": "Timeline",
    "text": "Timeline\nMini-Project 1 must be submitted on Moodle by 11:00 PM on Sun Sep 28.",
    "crumbs": [
      "Mini-Project 1: Text Analysis"
    ]
  },
  {
    "objectID": "miniproject01.html#topic-ideas",
    "href": "miniproject01.html#topic-ideas",
    "title": "Mini-Project 1: Text Analysis",
    "section": "Topic Ideas",
    "text": "Topic Ideas\n\nObama tweets\n\n#barack &lt;- read_csv(\"Data/tweets_potus.csv\") \nbarack &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/tweets_potus.csv\")\n#michelle &lt;- read_csv(\"Data/tweets_flotus.csv\") \nmichelle &lt;- read_csv(\"https://joeroith.github.io/264_spring_2025/Data/tweets_flotus.csv\")\n\ntweets &lt;- bind_rows(barack |&gt; \n                      mutate(person = \"Barack\"),\n                    michelle |&gt; \n                      mutate(person = \"Michelle\")) |&gt;\n  mutate(timestamp = ymd_hms(timestamp))\n\nPresident Barack Obama became the first US President with an official Twitter account, when @POTUS went live on May 18, 2015. (Yes, there was a time before Twitter/X.) First Lady Michelle Obama got in on Twitter much earlier, though her first tweet was not from @FLOTUS. All of the tweets from @POTUS and @FLOTUS are now archived on Twitter as @POTUS44 and @FLOTUS44, and they are available as a csv download from the National Archive. You can read more here.\nPotential things to investigate:\n\nuse of specific terms\nuse of @, #, RT (retweet), or -mo (personal tweet from Michelle Obama)\ntimestamp for date and time trends\nsentiment analysis\nanything else that seems interesting!\n\n\n\nDear Abby advice column\nRead in the “Dear Abby” data underlying The Pudding’s 30 Years of American Anxieties article.\n\nposts &lt;- read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")\n\nTake a couple minutes to scroll through the 30 Years of American Anxieties article to get ideas for themes that you might want to search for and illustrate using regular expressions.\n\n\nOther sources for string data\n\nOther articles from The Pudding\nNY Times headlines from the RTextTools package (see below)\nfurther analysis with the bigspotify data from class\nTidy Tuesday\nkaggle\nData Is Plural\nthe options are endless – be resourceful and creative!\n\n\nlibrary(RTextTools)  # may have to install first\n\nWarning: package 'RTextTools' was built under R version 4.5.1\n\ndata(NYTimes)\nas_tibble(NYTimes)\n\n# A tibble: 3,104 × 5\n   Article_ID Date      Title                                 Subject Topic.Code\n        &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;                                 &lt;fct&gt;        &lt;int&gt;\n 1      41246 1-Jan-96  Nation's Smaller Jails Struggle To C… Jails …         12\n 2      41257 2-Jan-96  FEDERAL IMPASSE SADDLING STATES WITH… Federa…         20\n 3      41268 3-Jan-96  Long, Costly Prelude Does Little To … Conten…         20\n 4      41279 4-Jan-96  Top Leader of the Bosnian Serbs Now … Bosnia…         19\n 5      41290 5-Jan-96  BATTLE OVER THE BUDGET: THE OVERVIEW… Battle…          1\n 6      41302 7-Jan-96  South African Democracy Stumbles on … politi…         19\n 7      41314 8-Jan-96  Among Economists, Little Fear on Def… econom…          1\n 8      41333 10-Jan-96 BATTLE OVER THE BUDGET: THE OVERVIEW… budget…          1\n 9      41344 11-Jan-96 High Court Is Cool To Census Change   census…         20\n10      41355 12-Jan-96 TURMOIL AT BARNEYS: THE DIFFICULTIES… barney…         15\n# ℹ 3,094 more rows",
    "crumbs": [
      "Mini-Project 1: Text Analysis"
    ]
  },
  {
    "objectID": "tech_setup.html",
    "href": "tech_setup.html",
    "title": "Tech Setup",
    "section": "",
    "text": "Ideally before class on Thurs Feb 6, and definitely before class on Tues Feb 11, you should follow these instructions to set up the software that we’ll be using throughout the semester. Even if you’ve already downloaded both R and RStudio, you’ll want to re-download to make sure that you have the most current versions.\n\nRequired: Download R and RStudio\n\nFIRST: Download R here.\n\nIn the top section, you will see three links “Download R for …”\nChoose the link that corresponds to your computer.\nAs of the start of this semester, the latest version of R is 4.4.2 (“Pile of Leaves”).\n\nSECOND: Download RStudio here.\n\nClick the button under step 2 to install the version of RStudio recommended for your computer.\nAs of the start of this semester, the latest version of RStudio is 2024.12.0 (Build 467).\n\nTHIRD: Check that when you go to File &gt; New Project &gt; New Directory, you see “Quarto Website” as an option.\n\n\nSuggested: Watch this video from Lisa Lendway at Macalester describing key configuration options for RStudio.\n\nSuggested: Change the default file download location for your internet browser.\n\nGenerally by default, internet browsers automatically save all files to the Downloads folder on your computer. In that case, you have to grab files from Downloads and move them to a more appropriate storage spot. You can change this option so that your browser asks you where to save each file before downloading it.\nThis page has information on how to do this for the most common browsers.\n\n\nRequired: Install required packages.\n\nAn R package is an extra bit of functionality that will help us in our data analysis efforts in a variety of ways. Many contributors create open source packages that can be added to base R to perform certain tasks in new and better ways.\nFor now, we’ll just make sure the tidyverse package is installed. Open RStudio and click on the Packages tab in the bottom right pane. Click the Install button and type “tidyverse” (without quotes) in the pop-up box. Click the Install button at the bottom of the pop-up box.\nYou will see a lot of text from status messages appearing in the Console as the packages are being installed. Wait until you see the &gt; again.\nEnter the command library(tidyverse) in the Console and hit Enter.\n\nQuit RStudio. You’re done setting up!\n\n\nOptional: For a refresher on RStudio features, watch this video. It also shows you how to customize the layout and color scheme of RStudio.",
    "crumbs": [
      "Tech Setup"
    ]
  }
]