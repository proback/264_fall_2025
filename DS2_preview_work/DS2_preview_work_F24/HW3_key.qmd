---
title: "HW3 Key"
format:
  pdf: default
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: FALSE

library(tidyverse)
library(stringr)
library(httr2)
library(httr)
library(tidycensus)
library(rvest)
library(polite)
```


# 07_apis.qmd

## On Your Own 2-3

```{r}
# Two ways to get my hidden key

myapikey <- readLines("~/264_fall_2024/DS2_preview_work/census_api_key.txt")

# I used the first line to store my CENSUS API key in .Renviron
#   after uncommenting - should only need to run one time
# Sys.setenv("CENSUS_KEY" = "my census api key pasted here")
my_census_api_key <- Sys.getenv("CENSUS_KEY")
```


2. Write a function to give choices about year, county, and variables

```{r}
# function to allow user inputs

MN_tract_data <- function(year, county, variables) {
  tidycensus::get_acs(
    Sys.sleep(0.5),
    year = year,
    state = "MN",
    geography = "tract",
    variables = variables,
    output = "wide",
    geometry = TRUE,
    county = county
  ) |>
    mutate(year = year)
}

# Should really build in checks so that county is in MN, year is in 
#   proper range, and variables are part of ACS1 data set

my_data <- MN_tract_data(year = 2021,
              county = "Hennepin", 
              variables = c("B01003_001", "B19013_001"))

ggplot(data = my_data) + 
  geom_sf(aes(fill = B01003_001E), colour = "white", linetype = 2)

my_data <- MN_tract_data(year = 2022,
              county = "Rice", 
              variables = c("B01003_001", "B19013_001"))

ggplot(data = my_data) + 
  geom_sf(aes(fill = B01003_001E), colour = "white", linetype = 2)

# Try other variables:
#  - B25077_001 is median home price
#  - B02001_002 is number of white residents
#  - etc.
# although the census codebook is admittedly quite daunting!
```


3. Use your function from (2) along with `map` and `list_rbind` to build a data set for Rice county for the years 2019-2021

```{r}
# To examine trends over time in Rice County
2019:2021 |>
  purrr::map(\(x) 
    MN_tract_data(
      x,
      county = "Rice", 
      variables = c("B01003_001", "B19013_001")
    )
  ) |>
  list_rbind()

# Or a little more simply
2019:2021 |>
  purrr::map(MN_tract_data,
             county = "Rice", 
             variables = c("B01003_001", "B19013_001")
            ) |>
  list_rbind()
```


## OMDB example

```{r}
#| eval: FALSE

myapikey <- readLines("~/264_fall_2024/DS2_preview_work/omdb_api_key.txt")

# I used the first line to store my OMDB API key in .Renviron
# Sys.setenv("OMDB_KEY" = "paste my omdb key here")
myapikey <- Sys.getenv("OMDB_KEY")

# Find url exploring examples at omdbapi.com
url <- str_c("http://www.omdbapi.com/?t=Coco&y=2017&apikey=", myapikey)

coco <- GET(url)   # coco holds response from server
coco               # Status of 200 is good!

details <- content(coco, "parse")   
details                         # get a list of 25 pieces of information
details$Year                    # how to access details
details[[2]]                    # since a list, another way to access
```

Now build a data set for a collection of movies

```{r}
#| eval: FALSE

# Must figure out pattern in URL for obtaining different movies
#  - try searching for others
movies <- c("Coco", "Wonder+Woman", "Get+Out", 
            "The+Greatest+Showman", "Thor:+Ragnarok")

# Set up empty tibble
omdb <- tibble(Title = character(), Rated = character(), Genre = character(),
       Actors = character(), Metascore = double(), imdbRating = double(),
       BoxOffice = double())

# Use for loop to run through API request process 5 times,
#   each time filling the next row in the tibble
#  - can do max of 1000 GETs per day
for(i in 1:5) {
  url <- str_c("http://www.omdbapi.com/?t=",movies[i],
               "&apikey=", myapikey)
  Sys.sleep(0.5)
  onemovie <- GET(url)
  details <- content(onemovie, "parse")
  omdb[i,1] <- details$Title
  omdb[i,2] <- details$Rated
  omdb[i,3] <- details$Genre
  omdb[i,4] <- details$Actors
  omdb[i,5] <- parse_number(details$Metascore)
  omdb[i,6] <- parse_number(details$imdbRating)
  omdb[i,7] <- parse_number(details$BoxOffice)   # no $ and ,'s
}

omdb

#  could use stringr functions to further organize this data - separate 
#    different genres, different actors, etc.
```

> Each person should have 5x5 tibble with different movies and different variables.


# 08_table_scraping.qmd

## On Your Own 2.2-2.4

2. We would like to create a tibble with 4 years of data (2001-2004) from the Minnesota Wild hockey team.  Specifically, we are interested in the "Scoring Regular Season" table from [this webpage](https://www.hockey-reference.com/teams/MIN/2001.html) and the similar webpages from 2002, 2003, and 2004.  Your final tibble should have 6 columns:  player, year, age, pos (position), gp (games played), and pts (points).

You should (a) write a function called `hockey_stats` with inputs for team and year to scrape data from the "scoring Regular Season" table, and (b) use iteration techniques to scrape and combine 4 years worth of data.  Here are some functions you might consider:

- `row_to_names(row_number = 1)` from the `janitor` package
- `clean_names()` also from the `janitor` package
- `bow()` and `scrape()` from the `polite` package
- `str_c()` from the `stringr` package (for creating urls with user inputs)
- `map2()` and `list_rbind()` for iterating and combining years

Try following these steps:

[SKIP] 1) Be sure you can find and clean the correct table from the 2021 season.

2) Organize your `rvest` code from (1) into functions from the `polite` package.

3) Place the code from (2) into a function where the user can input a team and year.  You would then adjust the url accordingly and produce a clean table for the user.

4) Use `map2` and `list_rbind` to build one data set containing Minnesota Wild data from 2001-2004.

```{r}
#| warning: FALSE

library(janitor)

robotstxt::paths_allowed("https://www.hockey-reference.com/teams/MIN/2001.html")
url <- str_c("https://www.hockey-reference.com/teams/MIN/2001.html")
player_url <- read_html(url)
player_stat <- html_nodes(player_url, css = "table")
html_table(player_stat, header = TRUE, fill = TRUE) 
html_table(player_stat, header = TRUE, fill = TRUE)[[4]]
html_table(player_stat, header = TRUE, fill = TRUE)[[4]] |>
  row_to_names(row_number = 1)

player_tibble <- html_table(player_stat, header = TRUE, fill = TRUE)[[4]] |>
  row_to_names(row_number = 1) |>
  clean_names()
player_tibble

# 2.2) perform the steps above with the polite package
session <- bow("https://www.hockey-reference.com/teams/MIN/2001.html", force = TRUE)

result <- scrape(session) |>
  html_nodes(css = "table") |>
  html_table(header = TRUE, fill = TRUE)
player_tibble <- result[[4]] |>
  row_to_names(row_number = 1) |>
  clean_names()
player_tibble

# 2.3) write function to scrape data from a single year for a specific team
hockey_stats <- function(team = "MIN", year = 2001) {
  url <- str_c("https://www.hockey-reference.com/teams/",team,"/",year,".html")
  session <- bow(url, force = TRUE)

  result <- scrape(session) |>
    html_nodes(css = "table") |>
    html_table(header = TRUE, fill = TRUE)
  player_tibble <- result[[4]] |>
    row_to_names(row_number = 1) |>
    clean_names() |>
    mutate(year = year) |>
    select(player, year, age, pos, gp, pts)
  player_tibble
}

hockey_stats("MIN", 2001)

# 2.4) use map function to scrape data from 2001 to 2004 and combine into 
#   a single data set
teams <- rep("MIN", 4)
years <- 2001:2004
temp <- map2(teams, years, hockey_stats)
hockey_data_4yrs <- list_rbind(temp) 
hockey_data_4yrs

```


# 09_web_scraping.qmd

## Pause to Ponder - 3 items on NIH News Releases right before the On Your Own section

**[Pause to Ponder:]** Create a function to scrape a single NIH press release page by filling missing pieces labeled `???`:

```{r}
# Helper function to reduce html_nodes() |> html_text() code duplication
get_text_from_page <- function(page, css_selector) {
    page |>
      html_nodes(css_selector) |>
      html_text()
}

# Main function to scrape and tidy desired attributes
scrape_page <- function(url) {
    Sys.sleep(2)
    page <- read_html(url)
    article_titles <- get_text_from_page(page, ".teaser-title")
    article_dates <- get_text_from_page(page, ".date-display-single")
    article_dates <- mdy(article_dates)
    article_description <- get_text_from_page(page, ".teaser-description")
    article_description <- str_trim(str_replace(article_description, 
                                                ".*\\n", 
                                                "")
                                    )
    
    tibble(
        title = article_titles,
        date = article_dates,
        description = article_description
    )
}
```

**[Pause to Ponder:]** Use a for loop over the first 5 pages:

```{r}
pages <- vector("list", length = 5)

for (i in 1:5) {
    base_url <- "https://www.nih.gov/news-events/news-releases"
    if (i==1) {
        url <- base_url
    } else {
        url <- str_c(base_url, "?page=", i-1)
    }
    pages[[i]] <- scrape_page(url)
}

df_articles <- bind_rows(pages)
head(df_articles)
```

**[Pause to Ponder:]** Use map functions in the purrr package:

```{r}
# Create a character vector of URLs for the first 5 pages
base_url <- "https://www.nih.gov/news-events/news-releases"
urls_all_pages <- c(base_url, str_c(base_url, "?page=", 1:4))

pages2 <- purrr::map(urls_all_pages, scrape_page)
df_articles2 <- bind_rows(pages2)
head(df_articles2)
```
