---
title: "MDSR2e Ch 13: Simulation"
output:
  pdf_document: default
  html_document:
    df_print: paged
execute:
  echo: true
  warning: false
  message: false
editor_options:
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false

# no longer needed:
# knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Goals

-   To

# Required reading

-   [Chapter 5](https://openintro-ims.netlify.app/explore-numerical.html) of your textbook

# New Code

-   `mosaic::favstats(dataset$var)`, provides summary statistics for variable `var` from `dataset`

# Before class

Let's start by loading a subset of data used for the story by doing the following command in R

```{r}
# Section 13.2: Grouping cancers

library(tidyverse)
library(mdsr)

# we want to use simulation to see if there's enough data - and enough relevant data - so that dimension reduction and statistical learning methods will work

# Find SD across 60 cell lines for each of 41078 probes from microarrays.  Low SD means there's nothing that probe can tell us about differences in cancers

NCI60 <- etl_NCI60()
spreads <- NCI60 %>%
  pivot_longer(
    -Probe, values_to = "expression", 
    names_to = "cellLine"
  ) %>%  
  group_by(Probe) %>%
  summarize(N = n(), spread = sd(expression)) %>%
  arrange(desc(spread)) %>%
  mutate(order = row_number())
spreads

# we want to pick only the top probes, but how many to choose??

# shuffle replaces each probe name with a random probe name, so that expression is no longer connected to cell line - i.e. probe expression data is meaningless (null hypothesis that probe labels are irrelevant is true)
sim_spreads <- NCI60 %>%
  pivot_longer(
    -Probe, values_to = "expression", 
    names_to = "cellLine"
  ) %>%
  mutate(Probe = mosaic::shuffle(Probe)) %>%
  group_by(Probe) %>%
  summarize(N = n(), spread = sd(expression)) %>%
  arrange(desc(spread)) %>%
  mutate(order = row_number())
sim_spreads

# compare real data to simulated data to see which probes provide evidence against the null hypothesis
spreads %>%
  filter(order <= 500) %>%
  ggplot(aes(x = order, y = spread)) +
  geom_line(color = "blue", size = 2) +
  geom_line(
    data = filter(sim_spreads, order <= 500), 
    color = "red", 
    size = 2
  ) +
  geom_text(
    label = "simulated", x = 275, y = 4.4, 
    size = 3, color = "red"
  ) +
  geom_text(
    label = "observed", x = 75, y = 5.5, 
    size = 3, color = "blue"
  )
# maybe stick with top 50 probes since those likely out of range of the null hypothesis


# Section 13.3: Randomizing functions

# uniform random number generator (between 0 and 1)
runif(5)

# select one value at random from a vector
select_one <- function(vec) {
  n <- length(vec)
  ind <- which.max(runif(n))
  vec[ind]
}
select_one(letters) # letters are a, b, c, ..., z
select_one(letters)

# can also sample from other distributions (e.g. rnorm(1))


# Section 13.4: Simulating variability

# Section 13.4.1: The partially-planned rendezvous (skip?  Although appears again in 13.6.3)

n <- 100000
sim_meet <- tibble(
  sally = runif(n, min = 0, max = 60),
  joan = runif(n, min = 0, max = 60),
  result = ifelse(
    abs(sally - joan) <= 10, "They meet", "They do not"
  )
)
mosaic::tally(~ result, format = "percent", data = sim_meet)

mosaic::binom.test(~result, n, success = "They meet", data = sim_meet)

ggplot(data = sim_meet, aes(x = joan, y = sally, color = result)) + 
  geom_point(alpha = 0.3) + 
  geom_abline(intercept = 10, slope = 1) + 
  geom_abline(intercept = -10, slope = 1) + 
  scale_color_brewer(palette = "Set2")

# Section 13.4.2: The jobs report (How not to be misled)

# simulate a year's worth of jobs reports with no real patterns
jobs_true <- 150
jobs_se <- 65  # in thousands of jobs
gen_samp <- function(true_mean, true_sd, 
                     num_months = 12, delta = 0, id = 1) {
  samp_year <- rep(true_mean, num_months) + 
    rnorm(num_months, mean = delta * (1:num_months), sd = true_sd)
  return(
    tibble(
      jobs_number = samp_year, 
      month = as.factor(1:num_months), 
      id = id
    )
  )
}

# params contains info about 3 simulations we want to run
n_sims <- 3
params <- tibble(
  sd = c(0, rep(jobs_se, n_sims)), 
  id = c("Truth", paste("Sample", 1:n_sims))
)
params

df <- params %>%
  pmap_dfr(~gen_samp(true_mean = jobs_true, true_sd = ..1, id = ..2))

ggplot(data = df, aes(x = month, y = jobs_number)) + 
  geom_hline(yintercept = jobs_true, linetype = 2) + 
  geom_col() + 
  facet_wrap(~ id) + 
  ylab("Number of new jobs (in thousands)")

# Section 13.4.3: Restaurant health and sanitation grades

# 0-13 = A, 14-27 = B, 28+ = C

minval <- 7
maxval <- 19
violation_scores <- Violations %>%
  filter(lubridate::year(inspection_date) == 2015) %>%
  filter(score >= minval & score <= maxval) %>%
  select(dba, score)

# an interesting pattern near the threshold of 13...
ggplot(data = violation_scores, aes(x = score)) + 
  geom_histogram(binwidth = 0.5) + 
  geom_vline(xintercept = 13, linetype = 2) + 
  scale_x_continuous(breaks = minval:maxval) + 
  annotate(
    "text", x = 10, y = 15000, 
    label = "'A' grade: score of 13 or less"
  )

# a simple simulation where scores of 13 and 14 are equally likely
scores <- mosaic::tally(~score, data = violation_scores)
scores

mean(scores[c("13", "14")])

random_flip <- 1:1000 %>%
  map_dbl(~mosaic::nflip(scores["13"] + scores["14"])) %>%
  enframe(name = "sim", value = "heads")
head(random_flip, 3)

# compare observed scores of 14 with expected number if 13 and 14 equally likely
ggplot(data = random_flip, aes(x = heads)) + 
  geom_histogram(binwidth = 10) + 
  geom_vline(xintercept = scores["14"], col = "red") + 
  annotate(
    "text", x = 2200, y = 75, 
    label = "observed", hjust = "left"
  ) + 
  xlab("Number of restaurants with scores of 14 (if equal probability)")


# Section 13.5: Random networks (see Ch 20)


# Section 13.6: Key principles of simulation

# Section 13.6.3: Reproducibility and random number seeds

# How many simulations should we do in random rendezvous problem?
campus_sim <- function(sims = 1000, wait = 10) {
  sally <- runif(sims, min = 0, max = 60)
  joan <- runif(sims, min = 0, max = 60)
  return(
    tibble(
      num_sims = sims, 
      meet = sum(abs(sally - joan) <= wait),
      meet_pct = meet / num_sims,
    )
  )
}

reps <- 5000
sim_results <- 1:reps %>%
  map_dfr(~map_dfr(c(100, 400, 1600), campus_sim))

sim_results %>%
  group_by(num_sims) %>%
  skim(meet_pct)

sim_results %>%
  ggplot(aes(x = meet_pct, color = factor(num_sims))) + 
  geom_density(size = 2) + 
  geom_vline(aes(xintercept = 11/36), linetype = 3) +
  scale_x_continuous("Proportion of times that Sally and Joan meet") + 
  scale_color_brewer("Number\nof sims", palette = "Set2")

# with 20000 simulations, sampling variability due to simulations is fairly negligible
1:reps %>%
  map_dfr(~campus_sim(20000)) %>%
  group_by(num_sims) %>%
  skim(meet_pct)

# seeds produce the same list of random values
set.seed(1974)
campus_sim()

campus_sim()

set.seed(1974)
campus_sim()

campus_sim()

```
