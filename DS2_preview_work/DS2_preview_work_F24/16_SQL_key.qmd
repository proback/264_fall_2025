---
title: "SQL: Exercises"
format:
  html: default
execute:
  echo: true
  warning: false
  message: false
editor_options:
  chunk_output_type: inline
---
  
You can download this .qmd file from [here](https://github.com/proback/264_fall_2024/blob/main/16_SQL_exercises.qmd).  Just hit the Download Raw File button.

The code in [15_SQL.qmd](https://github.com/proback/264_fall_2024/blob/main/SQL_code/15_SQL.qmd) walked us through many of the examples in MDSR Chapter 15; now, we present a set of practice exercises in converting from the tidyverse to SQL.

```{r}
#| message: false
#| warning: false

library(tidyverse)
library(mdsr)
library(dbplyr)
library(DBI)
```

```{r}
# connect to the database which lives on a remote server maintain by
#   St. Olaf's IT department
library(RMariaDB)
con <- dbConnect(
  MariaDB(), host = "mdb.stolaf.edu",
  user = "ruser", password = "ruserpass", 
  dbname = "flight_data"
)
```


## On Your Own - Extended Example from MDSR

Refer to [Section 15.5](https://mdsr-book.github.io/mdsr3e/15-sqlI.html#sec-ft8-flights) in MDSR, where they attempt to replicate some of FiveThirtyEight's analyses.  The MDSR authors provide a mix of SQL and R code to perform their analyses, but the code will not work if you simply cut-and-paste as-is into R.  Your task is to convert the book code into something that actually runs, and then *apply it to data from 2024*.  Very little of the code needs to be adjusted; it mostly needs to be repackaged.

Hints:

- use `dbGetQuery()`
- note that what they call `carrier` is just called `Reporting_Airline` in the `flightdata` table; you don't have to merge in a `carrier` table, although it's unfortunate that the `Reporting_Airline` codes are a bit cryptic


1. Below Figure 15.1, the MDSR authors first describe how to plot slowest and fastest airports.  Instead of using *target time*, which has a complex definition, we will use *arrival time*, which oversimplifies the situation but gets us in the ballpark.  Duplicate the equivalent of the table below for 2024 using the code in MDSR:

```{r}
#| eval: FALSE

# A tibble: 30 × 3
   dest  avgDepartDelay avgArrivalDelay
   <chr>          <dbl>           <dbl>
 1 ORD            14.3            13.1 
 2 MDW            12.8             7.40
 3 DEN            11.3             7.60
 4 IAD            11.3             7.45
 5 HOU            11.3             8.07
 6 DFW            10.7             9.00
 7 BWI            10.2             6.04
 8 BNA             9.47            8.94
 9 EWR             8.70            9.61
10 IAH             8.41            6.75
# 20 more rows
```


```{r}
dests <- dbGetQuery(con, '
SELECT
  dest, 
  SUM(1) AS numFlights,
  AVG(ArrDelay) AS avgArrivalDelay
FROM
  flightdata AS o
WHERE year = 2024
GROUP BY dest
ORDER BY numFlights DESC
LIMIT 0, 30
')

origins <- dbGetQuery(con, '
SELECT
  origin, 
  SUM(1) AS numFlights,
  AVG(ArrDelay) AS avgDepartDelay
FROM
  flightdata AS o
WHERE year = 2024
GROUP BY origin
ORDER BY numFlights DESC
LIMIT 0, 30
')

dests |>
  left_join(origins, by = c("dest" = "origin")) |>
  select(dest, avgDepartDelay, avgArrivalDelay) |>
  arrange(desc(avgDepartDelay))  |>
  as_tibble()
```

OR - use sql chunks:

```{sql, connection = con, output.var = "dests"}
SELECT
  dest, 
  SUM(1) AS numFlights,
  AVG(ArrDelay) AS avgArrivalDelay
FROM
  flightdata AS o
WHERE year = 2024
GROUP BY dest
ORDER BY numFlights DESC
LIMIT 0, 30;
```

```{sql, connection = con, output.var = "origins"}
SELECT
  origin, 
  SUM(1) AS numFlights,
  AVG(ArrDelay) AS avgDepartDelay
FROM
  flightdata AS o
WHERE year = 2024
GROUP BY origin
ORDER BY numFlights DESC
LIMIT 0, 30;
```

```{r}
dests |>
  left_join(origins, by = c("dest" = "origin")) |>
  select(dest, avgDepartDelay, avgArrivalDelay) |>
  arrange(desc(avgDepartDelay))  |>
  as_tibble()
```


2. Following the table above, the MDSR authors mimic one more FiveThirtyEight table which ranks carriers by time added vs. typical and time added vs. target.  In this case, we will find average arrival delay after controlling for the routes flown.  Again, duplicate the equivalent of the table below for 2024 using the code in MDSR:

```{r}
#| eval: FALSE

# A tibble: 14 × 5
   carrier carrier_name                numRoutes numFlights wAvgDelay
   <chr>   <chr>                           <int>      <dbl>     <dbl>
 1 VX      Virgin America                     72      57510   -2.69  
 2 FL      AirTran Airways Corporation       170      79495   -1.55  
 3 AS      Alaska Airlines Inc.              242     160257   -1.44  
 4 US      US Airways Inc.                   378     414665   -1.31  
 5 DL      Delta Air Lines Inc.              900     800375   -1.01  
 6 UA      United Air Lines Inc.             621     493528   -0.982 
 7 MQ      Envoy Air                         442     392701   -0.455 
 8 AA      American Airlines Inc.            390     537697   -0.0340
 9 HA      Hawaiian Airlines Inc.             56      74732    0.272 
10 OO      SkyWest Airlines Inc.            1250     613030    0.358 
11 B6      JetBlue Airways                   316     249693    0.767 
12 EV      ExpressJet Airlines Inc.         1534     686021    0.845 
13 WN      Southwest Airlines Co.           1284    1174633    1.13  
14 F9      Frontier Airlines Inc.            326      85474    2.29  
```


```{r}
# first find average arrival delay time by route (combination of dest 
# and origin)
routes <- dbGetQuery(con, '
SELECT 
  origin, dest, 
  SUM(1) AS numFlights,
  AVG(ArrDelay) AS avgDelay
FROM
  flightdata AS o
WHERE year = 2024
GROUP BY origin, dest
')
head(routes)

# Perform the same calculation but add carrier to group by
routes_carriers <- dbGetQuery(con, '
SELECT 
  origin, dest, Reporting_Airline,
  SUM(1) AS numFlights,
  AVG(ArrDelay) AS avgDelay
FROM
  flightdata AS o
WHERE year = 2024
GROUP BY origin, dest, Reporting_Airline
')

# merge the two previous data sets to add route averages to individual
# carriers within each route
routes_aug <- routes_carriers |>
  left_join(routes, by = c("origin" = "origin", "dest" = "dest")) |>
  as_tibble()
head(routes_aug)

# compute the difference between each carrier's performance and the 
# average performance on each route, and then calculate a weighted 
# average arrival delay for each carrier (weighted by number of flights)
routes_aug |>
  group_by(Reporting_Airline) |>
  summarize(
    numRoutes = n(), 
    numFlights = sum(numFlights.x), 
    wAvgDelay = sum(
      numFlights.x * (avgDelay.x - avgDelay.y), 
      na.rm = TRUE
    ) / sum(numFlights.x)
  ) |>
  arrange(wAvgDelay)
```


## On Your Own - Adapting 164 Code

These problems are based on class exercises from SDS 164, so you've already solved them in R!  Now we're going to try to duplicate those solutions in SQL (but with 2023 data instead of 2013).

```{r}
# Read in 2013 NYC flights data
library(nycflights13)
flights_nyc13 <- nycflights13::flights
planes_nyc13 <- nycflights13::planes
```


1. Summarize carriers flying to MSP by number of flights and proportion that are cancelled (assuming that a missing arrival time indicates a cancelled flight).  [This was #4 in 17_longer_pipelines.Rmd.]

```{r}
# Original solution from SDS 164
flights_nyc13 |>
  mutate(carrier = fct_collapse(carrier, "Delta +" = c("DL", "9E"), 
                                      "American +"= c("AA", "MQ"), 
                                     "United +" = c("EV", "OO", "UA"))) |>
  filter(dest == "MSP") |>   
  group_by(origin, carrier) |>
  summarize(n_flights = n(), 
            num_cancelled = sum(is.na(arr_time)),
            prop_cancelled = mean(is.na(arr_time)))
```

First duplicate the output above, then check trends in 2023 across all origins.  Here are a few hints:

- use flightdata instead of flights_nyc13
- remember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)
- is.na can be replaced with CASE WHEN ArrTime IS NULL THEN 1 ELSE 0 END or with CASE WHEN cancelled = 1 THEN 1 ELSE 0 END
- CASE WHEN can also be used replace fct_collapse

Duplicate 2013 NYC analysis for 2023:

```{sql, connection = con}
# Just looking at carriers - not needed in final solution
SELECT Reporting_Airline, 
  SUM(1) AS n_flights
FROM flightdata
WHERE year = 2023
GROUP BY Reporting_Airline
ORDER BY n_flights DESC;
```

```{sql, connection = con}
SELECT Reporting_Airline, dest, origin, Year, 
  SUM(1) AS n_flights,
  SUM(cancelled) AS num_cancelled,
  AVG(cancelled) AS prop_cancelled,
  CASE WHEN Reporting_Airline IN ("DL", "9E") THEN 'Delta +'
    WHEN Reporting_Airline IN ("AA", "MQ") THEN 'American +'
    WHEN Reporting_Airline IN ("EV", "OO", "UA") THEN 'United +'
    ELSE 'Other' END AS new_carrier
FROM flightdata
WHERE dest = "MSP" AND year = 2023 AND origin IN ("EWR", "JFK", "LGA")
GROUP BY origin, new_carrier
ORDER BY prop_cancelled DESC;
```


See trends in 2023 across all origins (similar for other two problems - just remove origin from WHERE and re-run):

```{sql, connection = con}
SELECT Reporting_Airline, dest, origin, Year, 
  SUM(1) AS n_flights,
  SUM(CASE WHEN ArrTime IS NULL THEN 1 ELSE 0 END) AS num_cancelled,
  AVG(CASE WHEN ArrTime IS NULL THEN 1 ELSE 0 END) AS prop_cancelled,
  CASE WHEN Reporting_Airline IN ("DL", "9E") THEN 'Delta +'
    WHEN Reporting_Airline IN ("AA", "MQ") THEN 'American +'
    WHEN Reporting_Airline IN ("EV", "OO", "UA") THEN 'United +'
    ELSE 'Other' END AS new_carrier
FROM flightdata
WHERE dest = "MSP" AND year = 2023
GROUP BY origin, new_carrier
ORDER BY prop_cancelled DESC;
```


2. Plot number of flights vs. proportion cancelled for every origin-destination pair (assuming that a missing arrival time indicates a cancelled flight).  [This was #7 in 17_longer_pipelines.Rmd.]

```{r}
# Original solution from SDS 164
flights_nyc13 |>
  group_by(origin, dest) |>
  summarize(n = n(),
            prop_cancelled = mean(is.na(arr_time))) |>
  filter(prop_cancelled < 1) |>
  ggplot(aes(n, prop_cancelled)) + 
  geom_point()
```

First duplicate the plot above for 2023 data, then check trends across all origins.  Do all of the data wrangling in SQL.  Here are a few hints:

- use flightdata instead of flights_nyc13
- remember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)
- use an `sql` chunk and an `r` chunk
- include `connection = ` and `output.var = ` in your sql chunk header (this doesn't seem to work with dbGetQuery()...)


Duplicate 2013 NYC analysis for 2023:

```{sql, connection = con, output.var = "plot_data"}
SELECT origin, dest,
  SUM(1) AS n,
  AVG(cancelled) AS prop_cancelled
FROM flightdata
WHERE year = 2023 AND (origin = "EWR" OR origin = "JFK" OR origin = "LGA")
GROUP BY origin, dest
HAVING prop_cancelled < 1
```

```{r}
plot_data |>
  ggplot(aes(n, prop_cancelled)) + 
  geom_point()
```

See trends in 2023 across all origins:

```{sql, connection = con, output.var = "plot_data2"}
SELECT origin, dest,
  SUM(1) AS n,
  AVG(cancelled) AS prop_cancelled
FROM flightdata
WHERE year = 2023
GROUP BY origin, dest
HAVING prop_cancelled < 1
```

```{r}
plot_data2 |>
  ggplot(aes(n, prop_cancelled)) + 
  geom_point()
```


3. Produce a table of weighted plane age by carrier, where weights are based on number of flights per plane.  [This was #6 in 26_more_joins.Rmd.]

```{r}
# Original solution from SDS 164
flights_nyc13 |>
  left_join(planes_nyc13, join_by(tailnum)) |>
  mutate(plane_age = 2013 - year.y) |>
  group_by(carrier) |>
  summarize(unique_planes = n_distinct(tailnum),
            mean_weighted_age = mean(plane_age, na.rm =TRUE),
            sd_weighted_age = sd(plane_age, na.rm =TRUE)) |>
  arrange(mean_weighted_age)
```

First duplicate the output above for 2023, then check trends across all origins.  Do all of the data wrangling in SQL.  Here are a few hints:

- use flightdata instead of flights_nyc13
- remember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)
- you'll have to merge the flights dataset with the planes dataset
- you can use DISTINCT inside a COUNT()
- investigate SQL clauses for calculating a standard deviation
- you cannot use a derived variable inside a summary clause in SELECT

For bonus points, also merge the airlines dataset and include the name of each carrier and not just the abbreviation!


Duplicate 2013 NYC analysis for 2023:

```{sql, connection = con}
SELECT * FROM planes LIMIT 1,6;
```

```{sql, connection = con}
SELECT * FROM flightdata LIMIT 0,10;
```

```{sql, connection = con}
SELECT * FROM airlines LIMIT 0,6;
```

```{sql, connection = con, output.var = "test"}
SELECT Reporting_Airline AS carrier, 
  a.name AS carrier_name,
  COUNT(DISTINCT o.TAIL_NUMBER) AS unique_planes,
  AVG(o.year - p.year) AS mean_weighted_age,
  STDDEV_SAMP(o.year - p.year) AS sd_weighted_age
FROM flightdata AS o
LEFT JOIN planes p ON o.TAIL_NUMBER = p.tailnum
LEFT JOIN airlines a ON o.Reporting_Airline = a.carrier
WHERE o.year = 2023 AND origin IN ("EWR", "JFK", "LGA")
GROUP BY carrier_name
ORDER BY mean_weighted_age ASC
```

```{r}
test
```

This could also be a place for subqueries.  Or possibly creating a temporary table:

```{sql, connection = con, output.var = "test3"}
CREATE TEMPORARY TABLE fd AS 
  SELECT reporting_airline, tail_number, year, origin
  FROM flightdata
  WHERE year = 2023 AND origin IN ("EWR", "JFK", "LGA");
```

```{sql, connection = con, output.var = "test3"}
SELECT Reporting_Airline AS carrier, 
  COUNT(DISTINCT o.TAIL_NUMBER) AS unique_planes,
  AVG(o.year - p.year) AS mean_weighted_age,
  STDDEV_SAMP(o.year - p.year) AS sd_weighted_age
FROM fd AS o
LEFT JOIN planes p ON o.TAIL_NUMBER = p.tailnum
GROUP BY carrier
ORDER BY mean_weighted_age ASC
```

```{r}
test3
```


See trends in 2023 across all origins:

```{sql, connection = con, output.var = "test2"}
SELECT Reporting_Airline AS carrier, 
  a.name AS carrier_name,
  COUNT(DISTINCT o.TAIL_NUMBER) AS unique_planes,
  AVG(o.year - p.year) AS mean_weighted_age,
  STDDEV_SAMP(o.year - p.year) AS sd_weighted_age
FROM flightdata AS o
LEFT JOIN planes p ON o.TAIL_NUMBER = p.tailnum
LEFT JOIN airlines a ON o.Reporting_Airline = a.carrier
WHERE o.year = 2023 
GROUP BY carrier_name
ORDER BY mean_weighted_age ASC
```

```{r}
test2
```


## On Your Own - Noninvasive Auditory Diagnostic Tools

You will use SQL to query the [Wideband Acoustic Immittance (WAI) Database](https://www.science.smith.edu/wai-database/) hosted by Smith College.  WAI measurements are being developed as noninvasive auditory diagnostic tools for people of all ages, and the WAI Database hosts WAI ear measurements that have been published in peer-review articles.  The goal of the database is to "enable auditory researchers to share WAI measurements and combine analyses over multiple datasets."

You have two primary goals:

1) duplicate Figure 1 from a [2019 manuscript](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7093226/) by Susan Voss.  You will need to query the WAI Database to build a dataset which you can pipe into `ggplot()` to recreate Figure 1 *as closely as possible*.

2) Find a study where subjects of different sex, race, ethnicity, or age groups were enrolled, and produce plots of frequency vs. mean absorption by group.

You should be using JOINs in both (1) and (2).

**Hints:**

- Parse the caption from Figure 1 carefully to determine how mean absorbances are calculated: "Mean absorbances for the 12 studies within the WAI database as of July 1, 2019. Noted in the legend are the peer-reviewed publications associated with the datasets, the number of individual ears, and the equipment used in the study. When multiple measurements were made on the same ear, the average from those measurements was used in the calculation across subjects for a given study. Some subjects have measurements on both a right and a left ear, and some subjects have measurements from only one ear; this figure includes every ear in the database and does not control for the effect of number of ears from each subject."
- filter for only the 12 studies shown in Figure 1 (and also for frequencies shown in Figure 1)
- study the patterns of frequencies.  It seems that most researchers used the same set of frequencies for each subject, ear, and session.
- note the scale of the x-axis
- the key labels contain AuthorsShortList, Year, and Instrument, in addition to the number of unique ears (I think Werner's N may be incorrect?)

**Starter Code for Part 1:**

```{r}
library(tidyverse)
library(mdsr)
library(dbplyr)
library(DBI)

library(RMariaDB)
con <- dbConnect(
  MariaDB(), host = "scidb.smith.edu",
  user = "waiuser", password = "smith_waiDB", 
  dbname = "wai"
)
Measurements <- tbl(con, "Measurements")
PI_Info <- tbl(con, "PI_Info")
Subjects <- tbl(con, "Subjects")

# collect(Measurements)
```

Run the following queries in a chunk with {sql, connection = con}:

- SHOW TABLES;
- DESCRIBE Measurements;
- SELECT * FROM PI_Info LIMIT 0,10;

```{sql, connection = con}
SHOW TABLES;
```

```{sql, connection = con}
DESCRIBE Measurements;
```

```{sql, connection = con}
SELECT * FROM PI_Info LIMIT 0,10;
```

Let's start to explore what this data looks like, starting with the Measurements table for one study:

Using Abur_2014 we can explore counts per subject/ear:

```{sql, connection = con}
SELECT Identifier, SubjectNumber, Session, Ear, Frequency
FROM Measurements
WHERE Identifier = 'Abur_2014' AND Frequency < 8000 AND Frequency > 200
  AND SubjectNumber = 1;
```

For Subject 1, there are 248 frequencies per session per ear (in the desired range), the frequencies are always the same, and 7 total sessions.  Thus, it appears any averaging must be across sessions.  We'll confirm some of these values below:

```{sql, connection = con}
SELECT SubjectNumber, Session, Ear,
  SUM(1) AS N,
  AVG(Absorbance) AS mean_absorbance
FROM Measurements
WHERE Identifier = 'Abur_2014' AND Frequency < 8000 AND Frequency > 200
  AND SubjectNumber IN (1, 3)
GROUP BY SubjectNumber, Session, Ear;
```

For Subjects 1 and 3, there are 248 frequencies per session per ear.


```{sql, connection = con}
# Note that variables can be used in WHERE but not SELECT
SELECT SubjectNumber, Ear, Frequency,
  SUM(1) AS N,
  AVG(Absorbance) AS mean_absorbance
FROM Measurements
WHERE Identifier = 'Abur_2014' AND Frequency = 1500
GROUP BY SubjectNumber, Ear, Frequency;
```

There are a variable number of sessions per subject (4-8).


```{sql, connection = con}
SELECT Frequency,
  SUM(1) AS N,
  AVG(Absorbance) AS mean_absorbance
FROM Measurements
WHERE Identifier = 'Abur_2014' AND Frequency < 8000 AND Frequency > 200
GROUP BY Frequency;
```

And it seems to always be the same 248 frequencies!


So let's create a data base with mean absorbance for each combination of study, subject, ear, and frequency:

```{sql, connection = con, output.var = "temp"}
SELECT Identifier, SubjectNumber, Ear, Frequency,
  SUM(1) AS N,
  AVG(Absorbance) AS mean_absorbance
FROM Measurements
WHERE Identifier IN ('Abur_2014', 'Feeney_2017', 'Groon_2015',
              'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006',
              'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010',
              'Werner_2010') AND Frequency < 8000 AND Frequency > 200
GROUP BY Identifier, SubjectNumber, Ear, Frequency;
```

```{r}
# 155103 x 6 data set
head(temp, 300)
```


This creates Figure 1 without the informative legend:

```{r}
#| warning: FALSE
#| message: FALSE  

temp |>
  mutate(logFrequency = log10(Frequency)) |>
  ggplot(aes(x = logFrequency, y = mean_absorbance, color = Identifier)) +
    geom_smooth() 
```

```{sql, connection = con, output.var = "temp2"}
SELECT p.Identifier, Year, AuthorsShortList, Instrument, 
       COUNT(DISTINCT SubjectNumber, Ear) AS Unique_Ears
FROM PI_Info AS p
LEFT JOIN Measurements AS m ON m.Identifier = p.Identifier
WHERE p.Identifier IN ('Abur_2014', 'Feeney_2017', 'Groon_2015',
              'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006',
              'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010',
              'Werner_2010') AND Frequency < 8000 AND Frequency > 200
GROUP BY p.Identifier, Instrument
```

```{r}
#| warning: FALSE
#| message: FALSE  

# use for smoother labels
temp2 <- temp2 |>
  mutate(our_label = str_glue("{AuthorsShortList} ({Year}) N={Unique_Ears}; {Instrument}"))
temp2

temp |>
  mutate(logFrequency = log10(Frequency)) |>
  ggplot(aes(x = logFrequency, y = mean_absorbance, color = Identifier)) +
    geom_smooth() +
    labs(y = "Mean Absorbance",
         x = "Frequency (Hz)",
         title = "Mean absorbance from each publication in WAI database") +
    scale_color_discrete(labels = temp2$our_label)
```



Find a study where subjects of different sex, race, or ethnicity were enrolled:

```{sql, connection = con, output.var = "temp0"}
SELECT * 
FROM Subjects 
WHERE Identifier = "Werner_2010";
```

```{r}
table(temp0$Sex)
table(temp0$AgeCategoryFirstMeasurement)
table(temp0$Race)
table(temp0$Ethnicity)
```

Then produce frequency vs mean absorbance plots by demographic category:

```{sql, connection = con, output.var = "temp3"}
SELECT s.Identifier, Sex, AgeCategoryFirstMeasurement, 
       s.SubjectNumber, Ear, Frequency, mean_absorbance
FROM Subjects AS s
RIGHT JOIN (SELECT m.Identifier, m.SubjectNumber, Ear, Frequency,
              AVG(Absorbance) AS mean_absorbance
            FROM Measurements AS m
            WHERE Identifier = 'Werner_2010' 
              AND Frequency < 8000 AND Frequency > 200
            GROUP BY m.Identifier, m.SubjectNumber, Ear, Frequency) AS j
  ON j.SubjectNumber = s.SubjectNumber AND
     j.Identifier = s.Identifier
```

```{r}
#| warning: FALSE
#| message: FALSE  

head(temp3)

temp3 |>
  mutate(logFrequency = log10(Frequency)) |>
  ggplot(aes(x = logFrequency, y = mean_absorbance, color = Sex)) +
    geom_smooth() +
    labs(y = "Mean Absorbance",
         x = "Frequency (Hz)",
         title = "Plots from Werner (2010) by Sex")

temp3 |>
  mutate(logFrequency = log10(Frequency)) |>
  ggplot(aes(x = logFrequency, y = mean_absorbance, 
             color = AgeCategoryFirstMeasurement)) +
    geom_smooth() +
    labs(y = "Mean Absorbance",
         x = "Frequency (Hz)",
         title = "Plots from Werner (2010) by Age",
         color = "Age Category")
```

